{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "라이브러리 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings \n",
    "warnings.filterwarnings('ignore') \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "from sklearn.svm import SVC\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier, ExtraTreesClassifier, VotingClassifier\n",
    "from sklearn.model_selection import KFold \n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier  \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "메모리 줄이는 전처리 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df):\n",
    "    start_mem_usg = df.memory_usage().sum() / 1024**2 \n",
    "    print(\"Memory usage of properties dataframe is :\",start_mem_usg,\" MB\")\n",
    "    NAlist = [] # 비어있는 값들 체크\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype != object:  \n",
    "            \n",
    "   \n",
    "            print(\"******************************\")\n",
    "            print(\"Column: \",col)\n",
    "            print(\"dtype before: \",df[col].dtype)\n",
    "            \n",
    "       \n",
    "            IsInt = False\n",
    "            mx = df[col].max()\n",
    "            mn = df[col].min()\n",
    "            \n",
    "       \n",
    "            if not np.isfinite(df[col]).all(): \n",
    "                NAlist.append(col)\n",
    "                df[col].fillna(mn-1,inplace=True)  \n",
    "                   \n",
    "            \n",
    "            asint = df[col].fillna(0).astype(np.int64)\n",
    "            result = (df[col] - asint)\n",
    "            result = result.sum()\n",
    "            if result > -0.01 and result < 0.01:\n",
    "                IsInt = True\n",
    "\n",
    "            if IsInt:\n",
    "                if mn >= 0:\n",
    "                    if mx < 255:\n",
    "                        df[col] = df[col].astype(np.uint8)\n",
    "                    elif mx < 65535:\n",
    "                        df[col] = df[col].astype(np.uint16)\n",
    "                    elif mx < 4294967295:\n",
    "                        df[col] = df[col].astype(np.uint32)\n",
    "                    else:\n",
    "                        df[col] = df[col].astype(np.uint64)\n",
    "                else:\n",
    "                    if mn > np.iinfo(np.int8).min and mx < np.iinfo(np.int8).max:\n",
    "                        df[col] = df[col].astype(np.int8)\n",
    "                    elif mn > np.iinfo(np.int16).min and mx < np.iinfo(np.int16).max:\n",
    "                        df[col] = df[col].astype(np.int16)\n",
    "                    elif mn > np.iinfo(np.int32).min and mx < np.iinfo(np.int32).max:\n",
    "                        df[col] = df[col].astype(np.int32)\n",
    "                    elif mn > np.iinfo(np.int64).min and mx < np.iinfo(np.int64).max:\n",
    "                        df[col] = df[col].astype(np.int64)    \n",
    "            \n",
    "           \n",
    "            else:\n",
    "                df[col] = df[col].astype(np.float32)\n",
    "            \n",
    "      \n",
    "            print(\"dtype after: \",df[col].dtype)\n",
    "            print(\"******************************\")\n",
    "    \n",
    "\n",
    "    print(\"___MEMORY USAGE AFTER COMPLETION:___\")\n",
    "    mem_usg = df.memory_usage().sum() / 1024**2 \n",
    "    print(\"Memory usage is: \",mem_usg,\" MB\")\n",
    "    print(\"This is \",100*mem_usg/start_mem_usg,\"% of the initial size\")\n",
    "    return df, NAlist\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "파일 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "regular_season_data=pd.read_csv(\"C:/Users/samsung/Desktop/WNCAA/WRegularSeasonDetailedResults.csv\")\n",
    "Teams=pd.read_csv(\"C:/Users/samsung/Desktop/WNCAA/WTeams.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019_3101_3113</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019_3101_3114</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019_3101_3120</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019_3101_3124</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019_3101_3125</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               ID  Pred\n",
       "0  2019_3101_3113   0.5\n",
       "1  2019_3101_3114   0.5\n",
       "2  2019_3101_3120   0.5\n",
       "3  2019_3101_3124   0.5\n",
       "4  2019_3101_3125   0.5"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Submission=pd.read_csv(\"C:/Users/samsung/Desktop/WNCAA/WSampleSubmissionStage2.csv\")\n",
    "Submission.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "메모리줄이기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of properties dataframe is : 13.380416870117188  MB\n",
      "******************************\n",
      "Column:  Season\n",
      "dtype before:  int64\n",
      "dtype after:  uint16\n",
      "******************************\n",
      "******************************\n",
      "Column:  DayNum\n",
      "dtype before:  int64\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "******************************\n",
      "Column:  WTeamID\n",
      "dtype before:  int64\n",
      "dtype after:  uint16\n",
      "******************************\n",
      "******************************\n",
      "Column:  WScore\n",
      "dtype before:  int64\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "******************************\n",
      "Column:  LTeamID\n",
      "dtype before:  int64\n",
      "dtype after:  uint16\n",
      "******************************\n",
      "******************************\n",
      "Column:  LScore\n",
      "dtype before:  int64\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "******************************\n",
      "Column:  NumOT\n",
      "dtype before:  int64\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "******************************\n",
      "Column:  WFGM\n",
      "dtype before:  int64\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "******************************\n",
      "Column:  WFGA\n",
      "dtype before:  int64\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "******************************\n",
      "Column:  WFGM3\n",
      "dtype before:  int64\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "******************************\n",
      "Column:  WFGA3\n",
      "dtype before:  int64\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "******************************\n",
      "Column:  WFTM\n",
      "dtype before:  int64\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "******************************\n",
      "Column:  WFTA\n",
      "dtype before:  int64\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "******************************\n",
      "Column:  WOR\n",
      "dtype before:  int64\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "******************************\n",
      "Column:  WDR\n",
      "dtype before:  int64\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "******************************\n",
      "Column:  WAst\n",
      "dtype before:  int64\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "******************************\n",
      "Column:  WTO\n",
      "dtype before:  int64\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "******************************\n",
      "Column:  WStl\n",
      "dtype before:  int64\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "******************************\n",
      "Column:  WBlk\n",
      "dtype before:  int64\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "******************************\n",
      "Column:  WPF\n",
      "dtype before:  int64\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "******************************\n",
      "Column:  LFGM\n",
      "dtype before:  int64\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "******************************\n",
      "Column:  LFGA\n",
      "dtype before:  int64\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "******************************\n",
      "Column:  LFGM3\n",
      "dtype before:  int64\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "******************************\n",
      "Column:  LFGA3\n",
      "dtype before:  int64\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "******************************\n",
      "Column:  LFTM\n",
      "dtype before:  int64\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "******************************\n",
      "Column:  LFTA\n",
      "dtype before:  int64\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "******************************\n",
      "Column:  LOR\n",
      "dtype before:  int64\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "******************************\n",
      "Column:  LDR\n",
      "dtype before:  int64\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "******************************\n",
      "Column:  LAst\n",
      "dtype before:  int64\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "******************************\n",
      "Column:  LTO\n",
      "dtype before:  int64\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "******************************\n",
      "Column:  LStl\n",
      "dtype before:  int64\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "******************************\n",
      "Column:  LBlk\n",
      "dtype before:  int64\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "******************************\n",
      "Column:  LPF\n",
      "dtype before:  int64\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "___MEMORY USAGE AFTER COMPLETION:___\n",
      "Memory usage is:  2.1645431518554688  MB\n",
      "This is  16.176948542534547 % of the initial size\n",
      "Memory usage of properties dataframe is : 0.0056610107421875  MB\n",
      "******************************\n",
      "Column:  TeamID\n",
      "dtype before:  int64\n",
      "dtype after:  uint16\n",
      "******************************\n",
      "___MEMORY USAGE AFTER COMPLETION:___\n",
      "Memory usage is:  0.003566741943359375  MB\n",
      "This is  63.00539083557951 % of the initial size\n"
     ]
    }
   ],
   "source": [
    "regular_season_data, NAlist_1 = reduce_mem_usage(regular_season_data)\n",
    "Teams, NAlist_2 = reduce_mem_usage(Teams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature extraction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "#True shooting percenatge feature를 생성하고 관련된 야투 와 자유투 행을 제거한다.\n",
    "regular_season_data[\"WTS\"]=((regular_season_data[\"WScore\"])/(2*(regular_season_data[\"WFGA\"]+(0.44*regular_season_data[\"WFTA\"]))))\n",
    "regular_season_data[\"LTS\"]=((regular_season_data[\"LScore\"])/(2*(regular_season_data[\"LFGA\"]+(0.44*regular_season_data[\"LFTA\"]))))\n",
    "\n",
    "#총리바운드개수 feature를 만들고 공격리바운드개수만 남긴다.\n",
    "regular_season_data[\"WTR\"]=(regular_season_data[\"WOR\"])+(regular_season_data[\"WDR\"])\n",
    "regular_season_data[\"LTR\"]=(regular_season_data[\"LOR\"])+(regular_season_data[\"LDR\"])\n",
    "regular_season_data.drop('WDR',axis=1,inplace=True)\n",
    "regular_season_data.drop('LDR',axis=1,inplace=True)\n",
    "\n",
    "#TOV% feature를 생성---100개의 posession당 평균턴오버갯수\n",
    "regular_season_data[\"WTOVP\"]=((regular_season_data[\"WTO\"])/(regular_season_data[\"WFGA\"]+(0.44*regular_season_data[\"WFTA\"])+regular_season_data[\"WTO\"]))*100\n",
    "regular_season_data[\"LTOVP\"]=((regular_season_data[\"LTO\"])/(regular_season_data[\"LFGA\"]+(0.44*regular_season_data[\"LFTA\"])+regular_season_data[\"LTO\"]))*100\n",
    "regular_season_data.drop('WTO',axis=1,inplace=True)\n",
    "regular_season_data.drop('LTO',axis=1,inplace=True)\n",
    "regular_season_data.drop('WFGM',axis=1,inplace=True)\n",
    "regular_season_data.drop('WFGA',axis=1,inplace=True)\n",
    "regular_season_data.drop('WFTM',axis=1,inplace=True)\n",
    "regular_season_data.drop('WFTA',axis=1,inplace=True)\n",
    "regular_season_data.drop('LFGM',axis=1,inplace=True)\n",
    "regular_season_data.drop('LFGA',axis=1,inplace=True)\n",
    "regular_season_data.drop('LFTM',axis=1,inplace=True)\n",
    "regular_season_data.drop('LFTA',axis=1,inplace=True)\n",
    "regular_season_data.drop('WPF',axis=1,inplace=True)\n",
    "regular_season_data.drop('LPF',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2010년 시즌기록으로 초기 전처리 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "regular_season_data_init=regular_season_data[regular_season_data[\"Season\"]==2010]\n",
    "\n",
    "#2010시즌 팀명단 dataframe만들기\n",
    "Teams_init=Teams\n",
    "Teams_1=pd.Series(list(set(regular_season_data_init[\"WTeamID\"]).intersection(set(Teams_init[\"TeamID\"]))))\n",
    "Teams_2=pd.Series(list(set(regular_season_data_init[\"LTeamID\"]).intersection(set(Teams_init[\"TeamID\"]))))\n",
    "Teams_init=pd.concat([Teams_1,Teams_2],ignore_index=True).drop_duplicates().reset_index(drop=True)\n",
    "Teams_init=pd.DataFrame(Teams_init)\n",
    "\n",
    "#2011~2018시즌기록과 팀명 추가하기\n",
    "for i in range(2011,2019):\n",
    "   \n",
    "    regular_season_data_add=regular_season_data[regular_season_data[\"Season\"]==i]\n",
    "    \n",
    "    Teams_1=pd.Series(list(set(regular_season_data_add[\"WTeamID\"]).intersection(set(Teams[\"TeamID\"]))))\n",
    "    Teams_2=pd.Series(list(set(regular_season_data_add[\"LTeamID\"]).intersection(set(Teams[\"TeamID\"]))))\n",
    "    Teams_add=pd.concat([Teams_1,Teams_2],ignore_index=True).drop_duplicates().reset_index(drop=True)\n",
    "    Teams_add=pd.DataFrame(Teams_add)\n",
    "    Teams_init=pd.concat([Teams_init,Teams_add])\n",
    "    regular_season_data_init=pd.concat([regular_season_data_init,regular_season_data_add])\n",
    "\n",
    "Teams_init.rename(columns={0:\"TeamID\"},inplace=True)\n",
    "\n",
    "#마진행 만들기\n",
    "regular_season_data_init[\"Margin\"]=regular_season_data_init[\"WScore\"]-regular_season_data_init[\"LScore\"]\n",
    "\n",
    "#홈,원정, 중립 지역 승률 행만들기, 2010년에대해\n",
    "WM=regular_season_data_init.groupby([\"Season\",\"WTeamID\"]).sum()[\"Margin\"].loc[2010,:]\n",
    "LM=regular_season_data_init.groupby([\"Season\",\"LTeamID\"]).sum()[\"Margin\"].loc[2010,:]\n",
    "\n",
    "WTS=regular_season_data_init.groupby([\"Season\",\"WTeamID\"]).sum()[\"WTS\"].loc[2010,:]\n",
    "LTS=regular_season_data_init.groupby([\"Season\",\"LTeamID\"]).sum()[\"LTS\"].loc[2010,:]\n",
    "WTR=regular_season_data_init.groupby([\"Season\",\"WTeamID\"]).sum()[\"WTR\"].loc[2010,:]\n",
    "LTR=regular_season_data_init.groupby([\"Season\",\"LTeamID\"]).sum()[\"LTR\"].loc[2010,:]\n",
    "WOR=regular_season_data_init.groupby([\"Season\",\"WTeamID\"]).sum()[\"WOR\"].loc[2010,:]\n",
    "LOR=regular_season_data_init.groupby([\"Season\",\"LTeamID\"]).sum()[\"LOR\"].loc[2010,:]\n",
    "WStl=regular_season_data_init.groupby([\"Season\",\"WTeamID\"]).sum()[\"WStl\"].loc[2010,:]\n",
    "LStl=regular_season_data_init.groupby([\"Season\",\"LTeamID\"]).sum()[\"LStl\"].loc[2010,:]\n",
    "WTOVP=regular_season_data_init.groupby([\"Season\",\"WTeamID\"]).sum()[\"WTOVP\"].loc[2010,:]\n",
    "LTOVP=regular_season_data_init.groupby([\"Season\",\"LTeamID\"]).sum()[\"LTOVP\"].loc[2010,:]\n",
    "\n",
    "WBlk=regular_season_data_init.groupby([\"Season\",\"WTeamID\"]).sum()[\"WBlk\"].loc[2010,:]\n",
    "LBlk=regular_season_data_init.groupby([\"Season\",\"LTeamID\"]).sum()[\"LBlk\"].loc[2010,:]\n",
    "WAst=regular_season_data_init.groupby([\"Season\",\"WTeamID\"]).sum()[\"WAst\"].loc[2010,:]\n",
    "LAst=regular_season_data_init.groupby([\"Season\",\"LTeamID\"]).sum()[\"LAst\"].loc[2010,:]\n",
    "HW=regular_season_data_init.groupby([\"Season\",\"WTeamID\",\"WLoc\"]).count()[\"DayNum\"].loc[2010,:,'H']\n",
    "HL=regular_season_data_init.groupby([\"Season\",\"LTeamID\",\"WLoc\"]).count()[\"DayNum\"].loc[2010,:,'A']\n",
    "AW=regular_season_data_init.groupby([\"Season\",\"WTeamID\",\"WLoc\"]).count()[\"DayNum\"].loc[2010,:,'A']\n",
    "AL=regular_season_data_init.groupby([\"Season\",\"LTeamID\",\"WLoc\"]).count()[\"DayNum\"].loc[2010,:,'H']\n",
    "NW=regular_season_data_init.groupby([\"Season\",\"WTeamID\",\"WLoc\"]).count()[\"DayNum\"].loc[2010,:,'N']\n",
    "NL=regular_season_data_init.groupby([\"Season\",\"LTeamID\",\"WLoc\"]).count()[\"DayNum\"].loc[2010,:,'N']\n",
    "HW.index.names=['Season', 'TeamID', 'WLoc']\n",
    "HL.index.names=['Season', 'TeamID', 'WLoc']\n",
    "WM.index.names=['Season','TeamID']\n",
    "LM.index.names=['Season','TeamID']\n",
    "\n",
    "#Margin\n",
    "WM.index.names=['Season','TeamID']\n",
    "LM.index.names=['Season','TeamID']\n",
    "WM=pd.DataFrame(WM)\n",
    "LM=pd.DataFrame(LM)\n",
    "M_result=pd.merge(WM,LM,on=\"TeamID\",how='outer').fillna(0)\n",
    "M_result.columns=[\"WM\",\"LM\"]\n",
    "M_result[\"Total_Margin\"]=M_result[\"WM\"]-M_result[\"LM\"]\n",
    "M_result.drop('WM',axis=1,inplace=True)\n",
    "M_result.drop('LM',axis=1,inplace=True)\n",
    "\n",
    "#Total shooting percentage\n",
    "WTS.index.names=['Season','TeamID']\n",
    "LTS.index.names=['Season','TeamID']\n",
    "WTS=pd.DataFrame(WTS)\n",
    "LTS=pd.DataFrame(LTS)\n",
    "TS_result=pd.merge(WTS,LTS,on=\"TeamID\",how='outer').fillna(0)\n",
    "TS_result.columns=[\"WTS\",\"LTS\"]\n",
    "TS_result[\"Total_TS\"]=TS_result[\"WTS\"]+TS_result[\"LTS\"]\n",
    "TS_result.drop('WTS',axis=1,inplace=True)\n",
    "TS_result.drop('LTS',axis=1,inplace=True)\n",
    "\n",
    "#Total Rebound \n",
    "WTR.index.names=['Season','TeamID']\n",
    "LTR.index.names=['Season','TeamID']\n",
    "WTR=pd.DataFrame(WTR)\n",
    "LTR=pd.DataFrame(LTR)\n",
    "TR_result=pd.merge(WTR,LTR,on=\"TeamID\",how='outer').fillna(0)\n",
    "TR_result.columns=[\"WTR\",\"LTR\"]\n",
    "TR_result[\"Total_TR\"]=TR_result[\"WTR\"]+TR_result[\"LTR\"]\n",
    "TR_result.drop('WTR',axis=1,inplace=True)\n",
    "TR_result.drop('LTR',axis=1,inplace=True)\n",
    "\n",
    "#Offensive Rebound \n",
    "WOR.index.names=['Season','TeamID']\n",
    "LOR.index.names=['Season','TeamID']\n",
    "WOR=pd.DataFrame(WOR)\n",
    "LOR=pd.DataFrame(LOR)\n",
    "OR_result=pd.merge(WOR,LOR,on=\"TeamID\",how='outer').fillna(0)\n",
    "OR_result.columns=[\"WOR\",\"LOR\"]\n",
    "OR_result[\"Total_OR\"]=OR_result[\"WOR\"]+OR_result[\"LOR\"]\n",
    "OR_result.drop('WOR',axis=1,inplace=True)\n",
    "OR_result.drop('LOR',axis=1,inplace=True)\n",
    "\n",
    "#Steal \n",
    "WStl.index.names=['Season','TeamID']\n",
    "LStl.index.names=['Season','TeamID']\n",
    "WStl=pd.DataFrame(WStl)\n",
    "LStl=pd.DataFrame(LStl)\n",
    "Stl_result=pd.merge(WStl,LStl,on=\"TeamID\",how='outer').fillna(0)\n",
    "Stl_result.columns=[\"WStl\",\"LStl\"]\n",
    "Stl_result[\"Total_Stl\"]=Stl_result[\"WStl\"]+Stl_result[\"LStl\"]\n",
    "Stl_result.drop('WStl',axis=1,inplace=True)\n",
    "Stl_result.drop('LStl',axis=1,inplace=True)\n",
    "\n",
    "#Turn over percentage\n",
    "WTOVP.index.names=['Season','TeamID']\n",
    "LTOVP.index.names=['Season','TeamID']\n",
    "WTOVP=pd.DataFrame(WTOVP)\n",
    "LTOVP=pd.DataFrame(LTOVP)\n",
    "TOVP_result=pd.merge(WTOVP,LTOVP,on=\"TeamID\",how='outer').fillna(0)\n",
    "TOVP_result.columns=[\"WTOVP\",\"LTOVP\"]\n",
    "TOVP_result[\"Total_TOVP\"]=TOVP_result[\"WTOVP\"]+TOVP_result[\"LTOVP\"]\n",
    "TOVP_result.drop('WTOVP',axis=1,inplace=True)\n",
    "TOVP_result.drop('LTOVP',axis=1,inplace=True)\n",
    "\n",
    "#Block\n",
    "WBlk.index.names=['Season','TeamID']\n",
    "LBlk.index.names=['Season','TeamID']\n",
    "WBlk=pd.DataFrame(WBlk)\n",
    "LBlk=pd.DataFrame(LBlk)\n",
    "Blk_result=pd.merge(WBlk,LBlk,on=\"TeamID\",how='outer').fillna(0)\n",
    "Blk_result.columns=[\"WBlk\",\"LBlk\"]\n",
    "Blk_result[\"Total_Blk\"]=Blk_result[\"WBlk\"]+Blk_result[\"LBlk\"]\n",
    "Blk_result.drop('WBlk',axis=1,inplace=True)\n",
    "Blk_result.drop('LBlk',axis=1,inplace=True)\n",
    "\n",
    "#Assist\n",
    "WAst.index.names=['Season','TeamID']\n",
    "LAst.index.names=['Season','TeamID']\n",
    "WAst=pd.DataFrame(WAst)\n",
    "LAst=pd.DataFrame(LAst)\n",
    "Ast_result=pd.merge(WAst,LAst,on=\"TeamID\",how='outer').fillna(0)\n",
    "Ast_result.columns=[\"WAst\",\"LAst\"]\n",
    "Ast_result[\"Total_Ast\"]=Ast_result[\"WAst\"]+Ast_result[\"LAst\"]\n",
    "Ast_result.drop('WAst',axis=1,inplace=True)\n",
    "Ast_result.drop('LAst',axis=1,inplace=True)\n",
    "\n",
    "\n",
    "HW=pd.DataFrame(HW)\n",
    "HL=pd.DataFrame(HL)\n",
    "H_result=pd.merge(HW,HL,on=\"TeamID\",how='outer').fillna(0)\n",
    "H_result.columns=[\"H_W\",\"H_L\"]\n",
    "H_result[\"H_Win_per\"]=(H_result[\"H_W\"])/(H_result[\"H_W\"]+H_result[\"H_L\"])\n",
    "AW.index.names=['Season', 'TeamID', 'WLoc']\n",
    "AL.index.names=['Season', 'TeamID', 'WLoc']\n",
    "AW=pd.DataFrame(AW)\n",
    "AL=pd.DataFrame(AL)\n",
    "A_result=pd.merge(AW,AL,on=\"TeamID\",how='outer').fillna(0)\n",
    "A_result.columns=[\"A_W\",\"A_L\"]\n",
    "A_result[\"A_Win_per\"]=(A_result[\"A_W\"])/(A_result[\"A_W\"]+A_result[\"A_L\"])\n",
    "NW.index.names=['Season', 'TeamID', 'WLoc']\n",
    "NL.index.names=['Season', 'TeamID', 'WLoc']\n",
    "NW=pd.DataFrame(NW)\n",
    "NL=pd.DataFrame(NL)\n",
    "N_result=pd.merge(NW,NL,on=\"TeamID\",how='outer').fillna(0)\n",
    "N_result.columns=[\"N_W\",\"N_L\"]\n",
    "N_result[\"N_Win_per\"]=(N_result[\"N_W\"])/(N_result[\"N_W\"]+N_result[\"N_L\"])\n",
    "Standings_init=pd.concat([H_result,A_result,N_result,M_result,Ast_result,Blk_result,TS_result,TR_result,OR_result,Stl_result,TOVP_result], axis=1).fillna(0)\n",
    "Standings_init[\"Season\"]=2010"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2011~2018년 기록을 전처리 하고 결합시킵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2011,2019):\n",
    "    WBlk=regular_season_data_init.groupby([\"Season\",\"WTeamID\"]).sum()[\"WBlk\"].loc[i,:]\n",
    "    LBlk=regular_season_data_init.groupby([\"Season\",\"LTeamID\"]).sum()[\"LBlk\"].loc[i,:]   \n",
    "    WAst=regular_season_data_init.groupby([\"Season\",\"WTeamID\"]).sum()[\"WAst\"].loc[i,:]\n",
    "    LAst=regular_season_data_init.groupby([\"Season\",\"LTeamID\"]).sum()[\"LAst\"].loc[i,:]   \n",
    "    WM=regular_season_data_init.groupby([\"Season\",\"WTeamID\"]).sum()[\"Margin\"].loc[i,:]\n",
    "    LM=regular_season_data_init.groupby([\"Season\",\"LTeamID\"]).sum()[\"Margin\"].loc[i,:]\n",
    "    HW=regular_season_data_init.groupby([\"Season\",\"WTeamID\",\"WLoc\"]).count()[\"DayNum\"].loc[i,:,'H']\n",
    "    HL=regular_season_data_init.groupby([\"Season\",\"LTeamID\",\"WLoc\"]).count()[\"DayNum\"].loc[i,:,'A']\n",
    "    AW=regular_season_data_init.groupby([\"Season\",\"WTeamID\",\"WLoc\"]).count()[\"DayNum\"].loc[i,:,'A']\n",
    "    AL=regular_season_data_init.groupby([\"Season\",\"LTeamID\",\"WLoc\"]).count()[\"DayNum\"].loc[i,:,'H']\n",
    "    NW=regular_season_data_init.groupby([\"Season\",\"WTeamID\",\"WLoc\"]).count()[\"DayNum\"].loc[i,:,'N']\n",
    "    NL=regular_season_data_init.groupby([\"Season\",\"LTeamID\",\"WLoc\"]).count()[\"DayNum\"].loc[i,:,'N']\n",
    "    WTS=regular_season_data_init.groupby([\"Season\",\"WTeamID\"]).sum()[\"WTS\"].loc[i,:]\n",
    "    LTS=regular_season_data_init.groupby([\"Season\",\"LTeamID\"]).sum()[\"LTS\"].loc[i,:]\n",
    "    WTR=regular_season_data_init.groupby([\"Season\",\"WTeamID\"]).sum()[\"WTR\"].loc[i,:]\n",
    "    LTR=regular_season_data_init.groupby([\"Season\",\"LTeamID\"]).sum()[\"LTR\"].loc[i,:]\n",
    "    WOR=regular_season_data_init.groupby([\"Season\",\"WTeamID\"]).sum()[\"WOR\"].loc[i,:]\n",
    "    LOR=regular_season_data_init.groupby([\"Season\",\"LTeamID\"]).sum()[\"LOR\"].loc[i,:]\n",
    "    WStl=regular_season_data_init.groupby([\"Season\",\"WTeamID\"]).sum()[\"WStl\"].loc[i,:]\n",
    "    LStl=regular_season_data_init.groupby([\"Season\",\"LTeamID\"]).sum()[\"LStl\"].loc[i,:]\n",
    "    WTOVP=regular_season_data_init.groupby([\"Season\",\"WTeamID\"]).sum()[\"WTOVP\"].loc[i,:]\n",
    "    LTOVP=regular_season_data_init.groupby([\"Season\",\"LTeamID\"]).sum()[\"LTOVP\"].loc[i,:]\n",
    "    HW.index.names=['Season', 'TeamID', 'WLoc']\n",
    "    HL.index.names=['Season', 'TeamID', 'WLoc']\n",
    "    \n",
    "    \n",
    "    WM.index.names=['Season','TeamID']\n",
    "    LM.index.names=['Season','TeamID']\n",
    "    WM=pd.DataFrame(WM)\n",
    "    LM=pd.DataFrame(LM)\n",
    "    M_result=pd.merge(WM,LM,on=\"TeamID\",how='outer').fillna(0)\n",
    "    M_result.columns=[\"WM\",\"LM\"]\n",
    "    M_result[\"Total_Margin\"]=M_result[\"WM\"]-M_result[\"LM\"]\n",
    "    M_result.drop('WM',axis=1,inplace=True)\n",
    "    M_result.drop('LM',axis=1,inplace=True)\n",
    "\n",
    "    #Total shooting percentage\n",
    "    WTS.index.names=['Season','TeamID']\n",
    "    LTS.index.names=['Season','TeamID']\n",
    "    WTS=pd.DataFrame(WTS)\n",
    "    LTS=pd.DataFrame(LTS)\n",
    "    TS_result=pd.merge(WTS,LTS,on=\"TeamID\",how='outer').fillna(0)\n",
    "    TS_result.columns=[\"WTS\",\"LTS\"]\n",
    "    TS_result[\"Total_TS\"]=TS_result[\"WTS\"]+TS_result[\"LTS\"]\n",
    "    TS_result.drop('WTS',axis=1,inplace=True)\n",
    "    TS_result.drop('LTS',axis=1,inplace=True)\n",
    "    \n",
    "    #Total Rebound \n",
    "    WTR.index.names=['Season','TeamID']\n",
    "    LTR.index.names=['Season','TeamID']\n",
    "    WTR=pd.DataFrame(WTR)\n",
    "    LTR=pd.DataFrame(LTR)\n",
    "    TR_result=pd.merge(WTR,LTR,on=\"TeamID\",how='outer').fillna(0)\n",
    "    TR_result.columns=[\"WTR\",\"LTR\"]\n",
    "    TR_result[\"Total_TR\"]=TR_result[\"WTR\"]+TR_result[\"LTR\"]\n",
    "    TR_result.drop('WTR',axis=1,inplace=True)\n",
    "    TR_result.drop('LTR',axis=1,inplace=True)\n",
    "\n",
    "    #Offensive Rebound \n",
    "    WOR.index.names=['Season','TeamID']\n",
    "    LOR.index.names=['Season','TeamID']\n",
    "    WOR=pd.DataFrame(WOR)\n",
    "    LOR=pd.DataFrame(LOR)\n",
    "    OR_result=pd.merge(WOR,LOR,on=\"TeamID\",how='outer').fillna(0)\n",
    "    OR_result.columns=[\"WOR\",\"LOR\"]\n",
    "    OR_result[\"Total_OR\"]=OR_result[\"WOR\"]+OR_result[\"LOR\"]\n",
    "    OR_result.drop('WOR',axis=1,inplace=True)\n",
    "    OR_result.drop('LOR',axis=1,inplace=True)\n",
    "        \n",
    "    #Steal \n",
    "    WStl.index.names=['Season','TeamID']\n",
    "    LStl.index.names=['Season','TeamID']\n",
    "    WStl=pd.DataFrame(WStl)\n",
    "    LStl=pd.DataFrame(LStl)\n",
    "    Stl_result=pd.merge(WStl,LStl,on=\"TeamID\",how='outer').fillna(0)\n",
    "    Stl_result.columns=[\"WStl\",\"LStl\"]\n",
    "    Stl_result[\"Total_Stl\"]=Stl_result[\"WStl\"]+Stl_result[\"LStl\"]\n",
    "    Stl_result.drop('WStl',axis=1,inplace=True)\n",
    "    Stl_result.drop('LStl',axis=1,inplace=True)\n",
    "    \n",
    "    #Turn over percentage\n",
    "    WTOVP.index.names=['Season','TeamID']\n",
    "    LTOVP.index.names=['Season','TeamID']\n",
    "    WTOVP=pd.DataFrame(WTOVP)\n",
    "    LTOVP=pd.DataFrame(LTOVP)\n",
    "    TOVP_result=pd.merge(WTOVP,LTOVP,on=\"TeamID\",how='outer').fillna(0)\n",
    "    TOVP_result.columns=[\"WTOVP\",\"LTOVP\"]\n",
    "    TOVP_result[\"Total_TOVP\"]=TOVP_result[\"WTOVP\"]+TOVP_result[\"LTOVP\"]\n",
    "    TOVP_result.drop('WTOVP',axis=1,inplace=True)\n",
    "    TOVP_result.drop('LTOVP',axis=1,inplace=True)\n",
    "    \n",
    "    WBlk.index.names=['Season','TeamID']\n",
    "    LBlk.index.names=['Season','TeamID']        \n",
    "    WBlk=pd.DataFrame(WBlk)\n",
    "    LBlk=pd.DataFrame(LBlk)\n",
    "    Blk_result=pd.merge(WBlk,LBlk,on=\"TeamID\",how='outer').fillna(0)\n",
    "    Blk_result.columns=[\"WBlk\",\"LBlk\"]\n",
    "    Blk_result[\"Total_Blk\"]=Blk_result[\"WBlk\"]-Blk_result[\"LBlk\"]\n",
    "    Blk_result.drop('WBlk',axis=1,inplace=True)\n",
    "    Blk_result.drop('LBlk',axis=1,inplace=True)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    WAst.index.names=['Season','TeamID']\n",
    "    LAst.index.names=['Season','TeamID']\n",
    "    WAst=pd.DataFrame(WAst)\n",
    "    LAst=pd.DataFrame(LAst)\n",
    "    Ast_result=pd.merge(WAst,LAst,on=\"TeamID\",how='outer').fillna(0)\n",
    "    Ast_result.columns=[\"WAst\",\"LAst\"]\n",
    "    Ast_result[\"Total_Ast\"]=Ast_result[\"WAst\"]+Ast_result[\"LAst\"]\n",
    "    Ast_result.drop('WAst',axis=1,inplace=True)\n",
    "    Ast_result.drop('LAst',axis=1,inplace=True)\n",
    "    \n",
    "    HW=pd.DataFrame(HW)\n",
    "    HL=pd.DataFrame(HL)\n",
    "    H_result=pd.merge(HW,HL,on=\"TeamID\",how='outer').fillna(0)\n",
    "    H_result.columns=[\"H_W\",\"H_L\"]\n",
    "    H_result[\"H_Win_per\"]=(H_result[\"H_W\"])/(H_result[\"H_W\"]+H_result[\"H_L\"])\n",
    "    AW.index.names=['Season', 'TeamID', 'WLoc']\n",
    "    AL.index.names=['Season', 'TeamID', 'WLoc']\n",
    "    AW=pd.DataFrame(AW)\n",
    "    AL=pd.DataFrame(AL)\n",
    "    A_result=pd.merge(AW,AL,on=\"TeamID\",how='outer').fillna(0)\n",
    "    A_result.columns=[\"A_W\",\"A_L\"]\n",
    "    A_result[\"A_Win_per\"]=(A_result[\"A_W\"])/(A_result[\"A_W\"]+A_result[\"A_L\"])\n",
    "    NW.index.names=['Season', 'TeamID', 'WLoc']\n",
    "    NL.index.names=['Season', 'TeamID', 'WLoc']\n",
    "    NW=pd.DataFrame(NW)\n",
    "    NL=pd.DataFrame(NL)\n",
    "    N_result=pd.merge(NW,NL,on=\"TeamID\",how='outer').fillna(0)\n",
    "    N_result.columns=[\"N_W\",\"N_L\"]\n",
    "    N_result[\"N_Win_per\"]=(N_result[\"N_W\"])/(N_result[\"N_W\"]+N_result[\"N_L\"])\n",
    "    a=pd.concat([H_result,A_result,N_result,M_result,Ast_result,Blk_result,TS_result,TR_result,OR_result,Stl_result,TOVP_result], axis=1).fillna(0)\n",
    "    a[\"Season\"]=i\n",
    "    Standings_init=pd.concat([Standings_init,a])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "블락,턴오등의 수치를 경기당 수치로 만들어줍니다. 또한, 승률을 기준으로 팀을 4개의 티어로 분류합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "Standings_init    \n",
    "Standings_init[\"T_Game\"]=Standings_init[\"H_W\"]+Standings_init[\"H_L\"]+Standings_init[\"A_W\"]+Standings_init[\"A_L\"]+Standings_init[\"N_W\"]+Standings_init[\"N_L\"]\n",
    "Standings_init[\"T_Win\"]=Standings_init[\"H_W\"]+Standings_init[\"A_W\"]+Standings_init[\"N_W\"]\n",
    "Standings_init[\"T_Lose\"]=Standings_init[\"H_L\"]+Standings_init[\"A_L\"]+Standings_init[\"N_L\"]\n",
    "Standings_init[\"T_WIn_Per\"]=Standings_init[\"T_Win\"]/Standings_init[\"T_Game\"]\n",
    "Standings_init[\"Margin_per_game\"]=Standings_init[\"Total_Margin\"]/Standings_init[\"T_Game\"]\n",
    "Standings_init[\"Ast_per_game\"]=Standings_init[\"Total_Ast\"]/Standings_init[\"T_Game\"]\n",
    "Standings_init[\"Blk_per_game\"]=Standings_init[\"Total_Blk\"]/Standings_init[\"T_Game\"]\n",
    "Standings_init[\"TS_per_game\"]=Standings_init[\"Total_TS\"]/Standings_init[\"T_Game\"]\n",
    "Standings_init[\"TR_per_game\"]=Standings_init[\"Total_TR\"]/Standings_init[\"T_Game\"]\n",
    "Standings_init[\"OR_per_game\"]=Standings_init[\"Total_OR\"]/Standings_init[\"T_Game\"]\n",
    "Standings_init[\"Stl_per_game\"]=Standings_init[\"Total_Stl\"]/Standings_init[\"T_Game\"]\n",
    "Standings_init[\"TOVP_per_game\"]=Standings_init[\"Total_TOVP\"]/Standings_init[\"T_Game\"]\n",
    "\n",
    "Teams_init=Teams_init.sort_values(by=[\"TeamID\"])\n",
    "Teams_init.index=Standings_init.index\n",
    "Teams_init=Teams_init.drop(columns=[\"TeamID\"],axis=1)\n",
    "Final_Standings=pd.concat([Teams_init,Standings_init],axis=1)\n",
    "Final_Standings=Final_Standings.sort_values(by=['Season',\"T_WIn_Per\",\"T_Win\"], ascending=False)\n",
    "Final_Standings[\"TeamID\"]=Final_Standings.index\n",
    "Final_Standings.index=range(len(Final_Standings))\n",
    "\n",
    "#전체승률 가지고 4개의 티어로 구분\n",
    "Final_Standings[\"Tier\"]=pd.cut(Final_Standings[\"T_WIn_Per\"],4,labels=[4,3,2,1])\n",
    "Standing=Final_Standings\n",
    "\n",
    "\n",
    "Standing.drop('H_W',axis=1,inplace=True)\n",
    "Standing.drop('H_L',axis=1,inplace=True)\n",
    "Standing.drop('A_W',axis=1,inplace=True)\n",
    "Standing.drop('A_L',axis=1,inplace=True)\n",
    "Standing.drop('N_W',axis=1,inplace=True)\n",
    "Standing.drop('N_L',axis=1,inplace=True)\n",
    "Standing.drop('T_Game',axis=1,inplace=True)\n",
    "Standing.drop('T_Win',axis=1,inplace=True)\n",
    "Standing.drop('T_Lose',axis=1,inplace=True)\n",
    "Standing.drop('N_Win_per',axis=1,inplace=True)\n",
    "Standing.drop(\"Total_Margin\",axis=1,inplace=True)\n",
    "Standing.drop(\"Total_Blk\",axis=1,inplace=True)\n",
    "Standing.drop(\"Total_Ast\",axis=1,inplace=True)\n",
    "Standing.drop(\"Total_TS\",axis=1,inplace=True)\n",
    "Standing.drop(\"Total_TR\",axis=1,inplace=True)\n",
    "Standing.drop(\"Total_OR\",axis=1,inplace=True)\n",
    "Standing.drop(\"Total_Stl\",axis=1,inplace=True)\n",
    "Standing.drop(\"Total_TOVP\",axis=1,inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>H_Win_per</th>\n",
       "      <th>A_Win_per</th>\n",
       "      <th>Season</th>\n",
       "      <th>T_WIn_Per</th>\n",
       "      <th>Margin_per_game</th>\n",
       "      <th>Ast_per_game</th>\n",
       "      <th>Blk_per_game</th>\n",
       "      <th>TS_per_game</th>\n",
       "      <th>TR_per_game</th>\n",
       "      <th>OR_per_game</th>\n",
       "      <th>Stl_per_game</th>\n",
       "      <th>TOVP_per_game</th>\n",
       "      <th>TeamID</th>\n",
       "      <th>Tier</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2018</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>36.906250</td>\n",
       "      <td>22.625000</td>\n",
       "      <td>5.187500</td>\n",
       "      <td>0.619224</td>\n",
       "      <td>41.687500</td>\n",
       "      <td>11.718750</td>\n",
       "      <td>10.687500</td>\n",
       "      <td>14.647277</td>\n",
       "      <td>3163</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2018</td>\n",
       "      <td>0.969697</td>\n",
       "      <td>26.303030</td>\n",
       "      <td>15.909091</td>\n",
       "      <td>3.575758</td>\n",
       "      <td>0.564302</td>\n",
       "      <td>37.575758</td>\n",
       "      <td>13.666667</td>\n",
       "      <td>8.696970</td>\n",
       "      <td>12.645131</td>\n",
       "      <td>3280</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>2018</td>\n",
       "      <td>0.968750</td>\n",
       "      <td>30.687500</td>\n",
       "      <td>20.343750</td>\n",
       "      <td>6.500000</td>\n",
       "      <td>0.581381</td>\n",
       "      <td>48.750000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>6.593750</td>\n",
       "      <td>15.109309</td>\n",
       "      <td>3124</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.941176</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>2018</td>\n",
       "      <td>0.941176</td>\n",
       "      <td>19.647059</td>\n",
       "      <td>16.617647</td>\n",
       "      <td>3.147059</td>\n",
       "      <td>0.564252</td>\n",
       "      <td>38.264706</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>8.205882</td>\n",
       "      <td>17.109863</td>\n",
       "      <td>3257</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.928571</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2018</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>16.437500</td>\n",
       "      <td>16.468750</td>\n",
       "      <td>3.312500</td>\n",
       "      <td>0.511875</td>\n",
       "      <td>34.781250</td>\n",
       "      <td>12.093750</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>11.735376</td>\n",
       "      <td>3273</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>2018</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>17.666667</td>\n",
       "      <td>18.303030</td>\n",
       "      <td>3.424242</td>\n",
       "      <td>0.575925</td>\n",
       "      <td>42.606061</td>\n",
       "      <td>12.424242</td>\n",
       "      <td>4.363636</td>\n",
       "      <td>15.372999</td>\n",
       "      <td>3125</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>2018</td>\n",
       "      <td>0.906250</td>\n",
       "      <td>18.562500</td>\n",
       "      <td>18.843750</td>\n",
       "      <td>3.125000</td>\n",
       "      <td>0.578500</td>\n",
       "      <td>41.843750</td>\n",
       "      <td>13.906250</td>\n",
       "      <td>8.531250</td>\n",
       "      <td>16.502017</td>\n",
       "      <td>3323</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>2018</td>\n",
       "      <td>0.906250</td>\n",
       "      <td>19.125000</td>\n",
       "      <td>14.718750</td>\n",
       "      <td>2.218750</td>\n",
       "      <td>0.521915</td>\n",
       "      <td>38.812500</td>\n",
       "      <td>11.875000</td>\n",
       "      <td>8.937500</td>\n",
       "      <td>16.555476</td>\n",
       "      <td>3453</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.941176</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>2018</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>18.823529</td>\n",
       "      <td>19.029412</td>\n",
       "      <td>3.382353</td>\n",
       "      <td>0.603590</td>\n",
       "      <td>38.147059</td>\n",
       "      <td>11.764706</td>\n",
       "      <td>7.558824</td>\n",
       "      <td>16.226196</td>\n",
       "      <td>3332</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>2018</td>\n",
       "      <td>0.870968</td>\n",
       "      <td>11.064516</td>\n",
       "      <td>15.677419</td>\n",
       "      <td>3.774194</td>\n",
       "      <td>0.586654</td>\n",
       "      <td>41.774194</td>\n",
       "      <td>12.709677</td>\n",
       "      <td>5.741935</td>\n",
       "      <td>17.181910</td>\n",
       "      <td>3141</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>2018</td>\n",
       "      <td>0.870968</td>\n",
       "      <td>17.290323</td>\n",
       "      <td>16.580645</td>\n",
       "      <td>1.322581</td>\n",
       "      <td>0.566421</td>\n",
       "      <td>35.903226</td>\n",
       "      <td>11.258065</td>\n",
       "      <td>9.064516</td>\n",
       "      <td>15.323728</td>\n",
       "      <td>3195</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>2018</td>\n",
       "      <td>0.843750</td>\n",
       "      <td>13.312500</td>\n",
       "      <td>18.468750</td>\n",
       "      <td>3.156250</td>\n",
       "      <td>0.515137</td>\n",
       "      <td>42.937500</td>\n",
       "      <td>14.937500</td>\n",
       "      <td>12.218750</td>\n",
       "      <td>17.481204</td>\n",
       "      <td>3138</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>2018</td>\n",
       "      <td>0.843750</td>\n",
       "      <td>13.656250</td>\n",
       "      <td>14.500000</td>\n",
       "      <td>2.250000</td>\n",
       "      <td>0.521515</td>\n",
       "      <td>39.718750</td>\n",
       "      <td>14.156250</td>\n",
       "      <td>9.531250</td>\n",
       "      <td>16.260168</td>\n",
       "      <td>3211</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>2018</td>\n",
       "      <td>0.843750</td>\n",
       "      <td>14.031250</td>\n",
       "      <td>17.093750</td>\n",
       "      <td>3.187500</td>\n",
       "      <td>0.509519</td>\n",
       "      <td>38.062500</td>\n",
       "      <td>12.218750</td>\n",
       "      <td>10.562500</td>\n",
       "      <td>14.486470</td>\n",
       "      <td>3346</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>2018</td>\n",
       "      <td>0.827586</td>\n",
       "      <td>13.655172</td>\n",
       "      <td>15.655172</td>\n",
       "      <td>3.413793</td>\n",
       "      <td>0.527238</td>\n",
       "      <td>39.586207</td>\n",
       "      <td>12.482759</td>\n",
       "      <td>8.068966</td>\n",
       "      <td>18.140465</td>\n",
       "      <td>3343</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>2018</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>13.969697</td>\n",
       "      <td>14.636364</td>\n",
       "      <td>2.909091</td>\n",
       "      <td>0.546984</td>\n",
       "      <td>40.848485</td>\n",
       "      <td>13.606061</td>\n",
       "      <td>8.636364</td>\n",
       "      <td>12.884642</td>\n",
       "      <td>3326</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>2018</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>6.937500</td>\n",
       "      <td>14.468750</td>\n",
       "      <td>2.031250</td>\n",
       "      <td>0.503413</td>\n",
       "      <td>34.875000</td>\n",
       "      <td>9.187500</td>\n",
       "      <td>7.343750</td>\n",
       "      <td>16.731555</td>\n",
       "      <td>3110</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>2018</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>6.968750</td>\n",
       "      <td>15.250000</td>\n",
       "      <td>1.750000</td>\n",
       "      <td>0.556591</td>\n",
       "      <td>31.687500</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.437500</td>\n",
       "      <td>17.786600</td>\n",
       "      <td>3294</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>2018</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>15.531250</td>\n",
       "      <td>15.031250</td>\n",
       "      <td>4.468750</td>\n",
       "      <td>0.553196</td>\n",
       "      <td>40.593750</td>\n",
       "      <td>12.687500</td>\n",
       "      <td>7.281250</td>\n",
       "      <td>16.819691</td>\n",
       "      <td>3376</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>2018</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>16.968750</td>\n",
       "      <td>16.500000</td>\n",
       "      <td>2.625000</td>\n",
       "      <td>0.532665</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>15.437500</td>\n",
       "      <td>8.687500</td>\n",
       "      <td>16.039608</td>\n",
       "      <td>3400</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>2018</td>\n",
       "      <td>0.806452</td>\n",
       "      <td>17.096774</td>\n",
       "      <td>15.354839</td>\n",
       "      <td>2.483871</td>\n",
       "      <td>0.546656</td>\n",
       "      <td>42.967742</td>\n",
       "      <td>15.419355</td>\n",
       "      <td>7.419355</td>\n",
       "      <td>15.213429</td>\n",
       "      <td>3199</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>2018</td>\n",
       "      <td>0.806452</td>\n",
       "      <td>10.645161</td>\n",
       "      <td>16.032258</td>\n",
       "      <td>3.903226</td>\n",
       "      <td>0.488170</td>\n",
       "      <td>39.806452</td>\n",
       "      <td>13.193548</td>\n",
       "      <td>8.612903</td>\n",
       "      <td>17.815591</td>\n",
       "      <td>3208</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>2018</td>\n",
       "      <td>0.806452</td>\n",
       "      <td>16.838710</td>\n",
       "      <td>16.838710</td>\n",
       "      <td>2.806452</td>\n",
       "      <td>0.567040</td>\n",
       "      <td>40.709677</td>\n",
       "      <td>12.322581</td>\n",
       "      <td>7.290323</td>\n",
       "      <td>15.928843</td>\n",
       "      <td>3355</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>2018</td>\n",
       "      <td>0.806452</td>\n",
       "      <td>9.903226</td>\n",
       "      <td>17.322581</td>\n",
       "      <td>2.612903</td>\n",
       "      <td>0.569869</td>\n",
       "      <td>35.193548</td>\n",
       "      <td>7.548387</td>\n",
       "      <td>7.483871</td>\n",
       "      <td>18.343275</td>\n",
       "      <td>3413</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>2018</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>9.166667</td>\n",
       "      <td>17.100000</td>\n",
       "      <td>2.233333</td>\n",
       "      <td>0.542873</td>\n",
       "      <td>37.600000</td>\n",
       "      <td>11.266667</td>\n",
       "      <td>5.800000</td>\n",
       "      <td>15.643024</td>\n",
       "      <td>3412</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>2018</td>\n",
       "      <td>0.793103</td>\n",
       "      <td>12.965517</td>\n",
       "      <td>18.034483</td>\n",
       "      <td>3.413793</td>\n",
       "      <td>0.537663</td>\n",
       "      <td>41.931034</td>\n",
       "      <td>12.896552</td>\n",
       "      <td>9.724138</td>\n",
       "      <td>17.362428</td>\n",
       "      <td>3123</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>2018</td>\n",
       "      <td>0.793103</td>\n",
       "      <td>12.241379</td>\n",
       "      <td>14.275862</td>\n",
       "      <td>2.551724</td>\n",
       "      <td>0.532964</td>\n",
       "      <td>43.241379</td>\n",
       "      <td>12.241379</td>\n",
       "      <td>4.724138</td>\n",
       "      <td>16.274347</td>\n",
       "      <td>3173</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>2018</td>\n",
       "      <td>0.793103</td>\n",
       "      <td>11.965517</td>\n",
       "      <td>14.344828</td>\n",
       "      <td>0.862069</td>\n",
       "      <td>0.576856</td>\n",
       "      <td>31.379310</td>\n",
       "      <td>8.724138</td>\n",
       "      <td>9.655172</td>\n",
       "      <td>16.569272</td>\n",
       "      <td>3377</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>2018</td>\n",
       "      <td>0.787879</td>\n",
       "      <td>12.545455</td>\n",
       "      <td>20.424242</td>\n",
       "      <td>1.545455</td>\n",
       "      <td>0.556668</td>\n",
       "      <td>39.818182</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>8.272727</td>\n",
       "      <td>15.784744</td>\n",
       "      <td>3177</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>2018</td>\n",
       "      <td>0.787879</td>\n",
       "      <td>8.363636</td>\n",
       "      <td>15.454545</td>\n",
       "      <td>1.303030</td>\n",
       "      <td>0.488007</td>\n",
       "      <td>34.272727</td>\n",
       "      <td>11.848485</td>\n",
       "      <td>9.818182</td>\n",
       "      <td>16.322064</td>\n",
       "      <td>3180</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>2018</td>\n",
       "      <td>0.787879</td>\n",
       "      <td>13.303030</td>\n",
       "      <td>15.424242</td>\n",
       "      <td>2.575758</td>\n",
       "      <td>0.544044</td>\n",
       "      <td>43.242424</td>\n",
       "      <td>13.484848</td>\n",
       "      <td>5.424242</td>\n",
       "      <td>15.408938</td>\n",
       "      <td>3378</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>2018</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>6.785714</td>\n",
       "      <td>13.964286</td>\n",
       "      <td>3.071429</td>\n",
       "      <td>0.486824</td>\n",
       "      <td>39.714286</td>\n",
       "      <td>12.250000</td>\n",
       "      <td>7.678571</td>\n",
       "      <td>20.974980</td>\n",
       "      <td>3126</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>2018</td>\n",
       "      <td>0.781250</td>\n",
       "      <td>13.937500</td>\n",
       "      <td>22.375000</td>\n",
       "      <td>2.218750</td>\n",
       "      <td>0.605873</td>\n",
       "      <td>39.281250</td>\n",
       "      <td>11.843750</td>\n",
       "      <td>8.843750</td>\n",
       "      <td>18.609031</td>\n",
       "      <td>3179</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>2018</td>\n",
       "      <td>0.781250</td>\n",
       "      <td>16.281250</td>\n",
       "      <td>17.062500</td>\n",
       "      <td>2.781250</td>\n",
       "      <td>0.540693</td>\n",
       "      <td>42.406250</td>\n",
       "      <td>15.593750</td>\n",
       "      <td>9.718750</td>\n",
       "      <td>15.930300</td>\n",
       "      <td>3268</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>2018</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>9.333333</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>1.629630</td>\n",
       "      <td>0.528688</td>\n",
       "      <td>35.370370</td>\n",
       "      <td>12.370370</td>\n",
       "      <td>9.814815</td>\n",
       "      <td>20.355418</td>\n",
       "      <td>3372</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>2018</td>\n",
       "      <td>0.774194</td>\n",
       "      <td>8.903226</td>\n",
       "      <td>15.096774</td>\n",
       "      <td>1.967742</td>\n",
       "      <td>0.494748</td>\n",
       "      <td>38.096774</td>\n",
       "      <td>15.225806</td>\n",
       "      <td>11.096774</td>\n",
       "      <td>20.315939</td>\n",
       "      <td>3107</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>2018</td>\n",
       "      <td>0.774194</td>\n",
       "      <td>13.870968</td>\n",
       "      <td>15.645161</td>\n",
       "      <td>2.258065</td>\n",
       "      <td>0.527052</td>\n",
       "      <td>41.903226</td>\n",
       "      <td>12.967742</td>\n",
       "      <td>9.612903</td>\n",
       "      <td>20.856249</td>\n",
       "      <td>3189</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.812500</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>2018</td>\n",
       "      <td>0.774194</td>\n",
       "      <td>10.322581</td>\n",
       "      <td>21.580645</td>\n",
       "      <td>2.774194</td>\n",
       "      <td>0.576872</td>\n",
       "      <td>42.322581</td>\n",
       "      <td>10.935484</td>\n",
       "      <td>5.870968</td>\n",
       "      <td>19.146347</td>\n",
       "      <td>3234</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>2018</td>\n",
       "      <td>0.774194</td>\n",
       "      <td>9.354839</td>\n",
       "      <td>15.967742</td>\n",
       "      <td>2.483871</td>\n",
       "      <td>0.574481</td>\n",
       "      <td>39.225806</td>\n",
       "      <td>9.935484</td>\n",
       "      <td>4.935484</td>\n",
       "      <td>19.566448</td>\n",
       "      <td>3281</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    H_Win_per  A_Win_per  Season  T_WIn_Per  Margin_per_game  Ast_per_game  \\\n",
       "0    1.000000   1.000000    2018   1.000000        36.906250     22.625000   \n",
       "1    1.000000   1.000000    2018   0.969697        26.303030     15.909091   \n",
       "2    1.000000   0.909091    2018   0.968750        30.687500     20.343750   \n",
       "3    0.941176   0.928571    2018   0.941176        19.647059     16.617647   \n",
       "4    0.928571   1.000000    2018   0.937500        16.437500     16.468750   \n",
       "5    0.923077   0.928571    2018   0.909091        17.666667     18.303030   \n",
       "6    1.000000   0.846154    2018   0.906250        18.562500     18.843750   \n",
       "7    1.000000   0.846154    2018   0.906250        19.125000     14.718750   \n",
       "8    0.941176   0.750000    2018   0.882353        18.823529     19.029412   \n",
       "9    0.846154   0.846154    2018   0.870968        11.064516     15.677419   \n",
       "10   1.000000   0.833333    2018   0.870968        17.290323     16.580645   \n",
       "11   1.000000   0.714286    2018   0.843750        13.312500     18.468750   \n",
       "12   0.857143   0.916667    2018   0.843750        13.656250     14.500000   \n",
       "13   0.923077   0.800000    2018   0.843750        14.031250     17.093750   \n",
       "14   0.833333   0.769231    2018   0.827586        13.655172     15.655172   \n",
       "15   0.866667   0.666667    2018   0.818182        13.969697     14.636364   \n",
       "16   1.000000   0.666667    2018   0.812500         6.937500     14.468750   \n",
       "17   0.857143   0.714286    2018   0.812500         6.968750     15.250000   \n",
       "18   0.866667   0.727273    2018   0.812500        15.531250     15.031250   \n",
       "19   0.875000   0.727273    2018   0.812500        16.968750     16.500000   \n",
       "20   0.928571   0.733333    2018   0.806452        17.096774     15.354839   \n",
       "21   0.823529   0.833333    2018   0.806452        10.645161     16.032258   \n",
       "22   0.857143   0.666667    2018   0.806452        16.838710     16.838710   \n",
       "23   0.928571   0.714286    2018   0.806452         9.903226     17.322581   \n",
       "24   0.769231   0.916667    2018   0.800000         9.166667     17.100000   \n",
       "25   0.866667   0.769231    2018   0.793103        12.965517     18.034483   \n",
       "26   0.923077   0.700000    2018   0.793103        12.241379     14.275862   \n",
       "27   0.916667   0.818182    2018   0.793103        11.965517     14.344828   \n",
       "28   0.833333   0.727273    2018   0.787879        12.545455     20.424242   \n",
       "29   1.000000   0.714286    2018   0.787879         8.363636     15.454545   \n",
       "30   0.928571   0.727273    2018   0.787879        13.303030     15.424242   \n",
       "31   0.769231   0.818182    2018   0.785714         6.785714     13.964286   \n",
       "32   0.800000   0.750000    2018   0.781250        13.937500     22.375000   \n",
       "33   0.800000   0.769231    2018   0.781250        16.281250     17.062500   \n",
       "34   0.909091   0.666667    2018   0.777778         9.333333     15.000000   \n",
       "35   0.923077   0.642857    2018   0.774194         8.903226     15.096774   \n",
       "36   1.000000   0.571429    2018   0.774194        13.870968     15.645161   \n",
       "37   0.812500   0.700000    2018   0.774194        10.322581     21.580645   \n",
       "38   0.857143   0.700000    2018   0.774194         9.354839     15.967742   \n",
       "\n",
       "    Blk_per_game  TS_per_game  TR_per_game  OR_per_game  Stl_per_game  \\\n",
       "0       5.187500     0.619224    41.687500    11.718750     10.687500   \n",
       "1       3.575758     0.564302    37.575758    13.666667      8.696970   \n",
       "2       6.500000     0.581381    48.750000    15.000000      6.593750   \n",
       "3       3.147059     0.564252    38.264706    12.000000      8.205882   \n",
       "4       3.312500     0.511875    34.781250    12.093750     10.000000   \n",
       "5       3.424242     0.575925    42.606061    12.424242      4.363636   \n",
       "6       3.125000     0.578500    41.843750    13.906250      8.531250   \n",
       "7       2.218750     0.521915    38.812500    11.875000      8.937500   \n",
       "8       3.382353     0.603590    38.147059    11.764706      7.558824   \n",
       "9       3.774194     0.586654    41.774194    12.709677      5.741935   \n",
       "10      1.322581     0.566421    35.903226    11.258065      9.064516   \n",
       "11      3.156250     0.515137    42.937500    14.937500     12.218750   \n",
       "12      2.250000     0.521515    39.718750    14.156250      9.531250   \n",
       "13      3.187500     0.509519    38.062500    12.218750     10.562500   \n",
       "14      3.413793     0.527238    39.586207    12.482759      8.068966   \n",
       "15      2.909091     0.546984    40.848485    13.606061      8.636364   \n",
       "16      2.031250     0.503413    34.875000     9.187500      7.343750   \n",
       "17      1.750000     0.556591    31.687500     8.000000      8.437500   \n",
       "18      4.468750     0.553196    40.593750    12.687500      7.281250   \n",
       "19      2.625000     0.532665    43.000000    15.437500      8.687500   \n",
       "20      2.483871     0.546656    42.967742    15.419355      7.419355   \n",
       "21      3.903226     0.488170    39.806452    13.193548      8.612903   \n",
       "22      2.806452     0.567040    40.709677    12.322581      7.290323   \n",
       "23      2.612903     0.569869    35.193548     7.548387      7.483871   \n",
       "24      2.233333     0.542873    37.600000    11.266667      5.800000   \n",
       "25      3.413793     0.537663    41.931034    12.896552      9.724138   \n",
       "26      2.551724     0.532964    43.241379    12.241379      4.724138   \n",
       "27      0.862069     0.576856    31.379310     8.724138      9.655172   \n",
       "28      1.545455     0.556668    39.818182    13.000000      8.272727   \n",
       "29      1.303030     0.488007    34.272727    11.848485      9.818182   \n",
       "30      2.575758     0.544044    43.242424    13.484848      5.424242   \n",
       "31      3.071429     0.486824    39.714286    12.250000      7.678571   \n",
       "32      2.218750     0.605873    39.281250    11.843750      8.843750   \n",
       "33      2.781250     0.540693    42.406250    15.593750      9.718750   \n",
       "34      1.629630     0.528688    35.370370    12.370370      9.814815   \n",
       "35      1.967742     0.494748    38.096774    15.225806     11.096774   \n",
       "36      2.258065     0.527052    41.903226    12.967742      9.612903   \n",
       "37      2.774194     0.576872    42.322581    10.935484      5.870968   \n",
       "38      2.483871     0.574481    39.225806     9.935484      4.935484   \n",
       "\n",
       "    TOVP_per_game  TeamID Tier  \n",
       "0       14.647277    3163    1  \n",
       "1       12.645131    3280    1  \n",
       "2       15.109309    3124    1  \n",
       "3       17.109863    3257    1  \n",
       "4       11.735376    3273    1  \n",
       "5       15.372999    3125    1  \n",
       "6       16.502017    3323    1  \n",
       "7       16.555476    3453    1  \n",
       "8       16.226196    3332    1  \n",
       "9       17.181910    3141    1  \n",
       "10      15.323728    3195    1  \n",
       "11      17.481204    3138    1  \n",
       "12      16.260168    3211    1  \n",
       "13      14.486470    3346    1  \n",
       "14      18.140465    3343    1  \n",
       "15      12.884642    3326    1  \n",
       "16      16.731555    3110    1  \n",
       "17      17.786600    3294    1  \n",
       "18      16.819691    3376    1  \n",
       "19      16.039608    3400    1  \n",
       "20      15.213429    3199    1  \n",
       "21      17.815591    3208    1  \n",
       "22      15.928843    3355    1  \n",
       "23      18.343275    3413    1  \n",
       "24      15.643024    3412    1  \n",
       "25      17.362428    3123    1  \n",
       "26      16.274347    3173    1  \n",
       "27      16.569272    3377    1  \n",
       "28      15.784744    3177    1  \n",
       "29      16.322064    3180    1  \n",
       "30      15.408938    3378    1  \n",
       "31      20.974980    3126    1  \n",
       "32      18.609031    3179    1  \n",
       "33      15.930300    3268    1  \n",
       "34      20.355418    3372    1  \n",
       "35      20.315939    3107    1  \n",
       "36      20.856249    3189    1  \n",
       "37      19.146347    3234    1  \n",
       "38      19.566448    3281    1  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Standing.head(39)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "제출해야하는 파일 형식( Team1 과 Team2) 에맞게 Team1과 Team2의 기록들을 붙입니다.(훈련데이터셋 완성하기) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of properties dataframe is : 0.0808258056640625  MB\n",
      "******************************\n",
      "Column:  Season\n",
      "dtype before:  int64\n",
      "dtype after:  uint16\n",
      "******************************\n",
      "******************************\n",
      "Column:  DayNum\n",
      "dtype before:  int64\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "******************************\n",
      "Column:  WTeamID\n",
      "dtype before:  int64\n",
      "dtype after:  uint16\n",
      "******************************\n",
      "******************************\n",
      "Column:  WScore\n",
      "dtype before:  int64\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "******************************\n",
      "Column:  LTeamID\n",
      "dtype before:  int64\n",
      "dtype after:  uint16\n",
      "******************************\n",
      "******************************\n",
      "Column:  LScore\n",
      "dtype before:  int64\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "******************************\n",
      "Column:  NumOT\n",
      "dtype before:  int64\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "___MEMORY USAGE AFTER COMPLETION:___\n",
      "Memory usage is:  0.022787094116210938  MB\n",
      "This is  28.192845006607513 % of the initial size\n",
      "Memory usage of properties dataframe is : 0.1538848876953125  MB\n",
      "******************************\n",
      "Column:  Pred\n",
      "dtype before:  float64\n",
      "dtype after:  float32\n",
      "******************************\n",
      "___MEMORY USAGE AFTER COMPLETION:___\n",
      "Memory usage is:  0.1154327392578125  MB\n",
      "This is  75.01239464551314 % of the initial size\n"
     ]
    }
   ],
   "source": [
    "NCAASeed=pd.read_csv(\"C:/Users/samsung/Desktop/WNCAA/WNCAATourneySeeds.csv\")\n",
    "NCAASeed[\"Seed\"]=NCAASeed['Seed'].str.extract('(\\d+)').astype(int) # 숫자만 추출해서 정수로 만들기.\n",
    "a=list(NCAASeed[NCAASeed[\"Season\"]==2010][\"TeamID\"])\n",
    "c_init=pd.DataFrame(list(combinations(a,2)))\n",
    "c_init.columns=['TeamID1','TeamID2']\n",
    "t=c_init.min(axis=1)\n",
    "h=c_init.max(axis=1)\n",
    "c_init[\"TeamID1\"]=t\n",
    "c_init[\"TeamID2\"]=h\n",
    "c_init=c_init.sort_values(by=[\"TeamID1\",'TeamID2'])\n",
    "c_init[\"Season\"]=np.zeros(len(c_init[\"TeamID1\"]))\n",
    "c_init=pd.DataFrame(c_init)\n",
    "c_init=c_init.replace(0,2010)\n",
    "for i in range(2011,2019):\n",
    "    ab=list(NCAASeed[NCAASeed[\"Season\"]==i][\"TeamID\"])\n",
    "    c_add=pd.DataFrame(list(combinations(ab,2)))\n",
    "    c_add.columns=['TeamID1','TeamID2']\n",
    "    tk=c_add.min(axis=1)\n",
    "    hm=c_add.max(axis=1)\n",
    "    c_add[\"TeamID1\"]=tk\n",
    "    c_add[\"TeamID2\"]=hm\n",
    "    c_add=c_add.sort_values(by=[\"TeamID1\",'TeamID2'])\n",
    "    c_add[\"Season\"]=np.zeros(len(c_add[\"TeamID1\"]))\n",
    "    c_add=pd.DataFrame(c_add)\n",
    "    c_add=c_add.replace(0,i)\n",
    "    c_init=pd.concat([c_init,c_add])\n",
    "\n",
    "tournament=pd.read_csv(\"C:/Users/samsung/Desktop/WNCAA/WNCAATourneyCompactResults.csv\")\n",
    "sample=pd.read_csv(\"C:/Users/samsung/Desktop/WNCAA/WSampleSubmissionStage1.csv\")\n",
    "tournament, NAlist_3 = reduce_mem_usage(tournament)\n",
    "sample, NAlist_4 = reduce_mem_usage(sample)\n",
    "\n",
    "Train_tournament=tournament\n",
    "Train_tournament[\"Output\"]=np.zeros(len(Train_tournament.index))\n",
    "\n",
    "Train_tournament[\"Output\"]=(Train_tournament[\"WTeamID\"]<Train_tournament[\"LTeamID\"]).astype('int64')\n",
    "a=pd.concat([Train_tournament[\"WTeamID\"],Train_tournament[\"LTeamID\"]],axis=1)\n",
    "Train_tournament[\"TeamID1\"]=a.min(axis=1)\n",
    "Train_tournament[\"TeamID2\"]=a.max(axis=1)\n",
    "Train_tournament.drop('WTeamID',axis=1,inplace=True)\n",
    "Train_tournament.drop('LTeamID',axis=1,inplace=True)\n",
    "Train_tournament.drop('WLoc',axis=1,inplace=True)\n",
    "Train_tournament.drop('NumOT',axis=1,inplace=True)\n",
    "Train_tournament.drop('WScore',axis=1,inplace=True)\n",
    "Train_tournament.drop('LScore',axis=1,inplace=True)\n",
    "Train_tournament=Train_tournament.sort_values([\"Season\",\"TeamID1\",\"TeamID2\"])\n",
    "c_init.index=range(len(c_init[\"Season\"]))\n",
    "\n",
    "\n",
    "Train_tournament.index=range(len(Train_tournament[\"Season\"]))\n",
    "target_train=pd.merge(c_init,Train_tournament).drop('DayNum',axis=1)\n",
    "Standing=Standing.sort_values([\"Season\",\"TeamID\"])\n",
    "Standing=pd.merge(Standing,NCAASeed)\n",
    "\n",
    "Standing.rename(columns={\"TeamID\":\"TeamID1\"}, inplace = True)\n",
    "Standing_1=Standing\n",
    "Final=pd.merge(Standing_1,target_train)\n",
    "Final.columns\n",
    "Final.rename(columns={\"Stl_per_game\":\"T1_Stl_per_game\",\"TOVP_per_game\":\"T1_TOVP_per_game\",\"OR_per_game\":\"T1_OR_per_game\",\"TR_per_game\":\"T1_TR_per_game\",\"TS_per_game\":\"T1_TS_per_game\",\"Blk_per_game\":\"T1_Blk_per_game\",\"Ast_per_game\":\"T1_Ast_per_game\",\"Margin_per_game\":\"T1_Margin_per_game\",\"Seed\":\"T1_Seed\",\"H_Win_per\":\"T1_H_Win_per\",\"A_Win_per\":\"T1_A_Win_per\",\"T_WIn_Per\":\"T1_T_WIn_Per\",\"Tier\":\"T1_Tier\"}, inplace = True)\n",
    "Standing_1.rename(columns={\"TeamID1\":\"TeamID2\"}, inplace = True)\n",
    "\n",
    "\n",
    "Final=pd.merge(Final,Standing_1)\n",
    "Final.rename(columns={\"Stl_per_game\":\"T2_Stl_per_game\",\"TOVP_per_game\":\"T2_TOVP_per_game\",\"OR_per_game\":\"T2_OR_per_game\",\"TR_per_game\":\"T2_TR_per_game\",\"TS_per_game\":\"T2_TS_per_game\",\"Blk_per_game\":\"T2_Blk_per_game\",\"Ast_per_game\":\"T2_Ast_per_game\",\"Margin_per_game\":\"T2_Margin_per_game\",\"Seed\":\"T2_Seed\",\"H_Win_per\":\"T2_H_Win_per\",\"A_Win_per\":\"T2_A_Win_per\",\"T_WIn_Per\":\"T2_T_WIn_Per\",\"Tier\":\"T2_Tier\"}, inplace = True)\n",
    "Final=Final.sort_values([\"Season\",\"TeamID1\",'TeamID2'])\n",
    "Final.index=range(len(Final[\"Season\"]))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Final.drop('TeamID1',axis=1,inplace=True)\n",
    "Final.drop('TeamID2',axis=1,inplace=True)\n",
    "Final[\"T1_T_WIn_Per\"]=Final[\"T1_T_WIn_Per\"].astype(np.float64)\n",
    "Final[\"T2_T_WIn_Per\"]=Final[\"T2_T_WIn_Per\"].astype(np.float64)\n",
    "Final[\"T1_Seed\"]=Final[\"T1_Seed\"].astype(np.float64)\n",
    "Final[\"T2_Seed\"]=Final[\"T2_Seed\"].astype(np.float64)\n",
    "Final[\"T1_Tier\"]=Final[\"T1_Tier\"].astype(np.float64)\n",
    "Final[\"T2_Tier\"]=Final[\"T2_Tier\"].astype(np.float64)\n",
    "Final['T1_Margin_per_game']=Final[\"T1_Margin_per_game\"].astype(np.float64)\n",
    "Final['T2_Margin_per_game']=Final[\"T2_Margin_per_game\"].astype(np.float64)\n",
    "Final['T1_TS_per_game']=Final['T1_TS_per_game'].astype(np.float64)\n",
    "Final['T2_TS_per_game']=Final['T2_TS_per_game'].astype(np.float64)\n",
    "Final['T1_TR_per_game']=Final['T1_TR_per_game'].astype(np.float64)\n",
    "Final['T2_TR_per_game']=Final['T2_TR_per_game'].astype(np.float64)\n",
    "Final['T1_OR_per_game']=Final['T1_OR_per_game'].astype(np.float64)\n",
    "Final['T2_OR_per_game']=Final['T2_OR_per_game'].astype(np.float64)\n",
    "Final['T1_Stl_per_game']=Final['T1_Stl_per_game'].astype(np.float64)\n",
    "Final['T2_Stl_per_game']=Final['T2_Stl_per_game'].astype(np.float64)\n",
    "Final['T1_TOVP_per_game']=Final['T1_TOVP_per_game'].astype(np.float64)\n",
    "Final['T2_TOVP_per_game']=Final['T2_TOVP_per_game'].astype(np.float64)\n",
    "\n",
    "Final['T1_Ast_per_game']=Final[\"T1_Ast_per_game\"].astype(np.float64)\n",
    "Final['T2_Ast_per_game']=Final[\"T2_Ast_per_game\"].astype(np.float64)\n",
    "Final.drop('Season',axis=1,inplace=True)\n",
    "Output=pd.DataFrame(Final[\"Output\"])\n",
    "Final.drop('Output',axis=1,inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2019년 기록을 갖고 위 식을 반복(X_test 만들기)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "regular_season_data_2019=regular_season_data[regular_season_data[\"Season\"]==2019]\n",
    "\n",
    "Teams_2019=Teams\n",
    "Teams_1=pd.Series(list(set(regular_season_data_2019[\"WTeamID\"]).intersection(set(Teams_2019[\"TeamID\"]))))\n",
    "    \n",
    "Teams_2=pd.Series(list(set(regular_season_data_2019[\"LTeamID\"]).intersection(set(Teams_2019[\"TeamID\"]))))\n",
    "Teams_2019=pd.concat([Teams_1,Teams_2],ignore_index=True).drop_duplicates().reset_index(drop=True)\n",
    "Teams_2019=pd.DataFrame(Teams_2019)\n",
    "Teams_2019.rename(columns={0:\"TeamID\"},inplace=True)\n",
    "regular_season_data_2019[\"Margin\"]=regular_season_data_2019[\"WScore\"]-regular_season_data_2019[\"LScore\"]\n",
    "\n",
    "#홈,원정, 중립 지역 승률 행만들기\n",
    "\n",
    "WM=regular_season_data_2019.groupby([\"Season\",\"WTeamID\"]).sum()[\"Margin\"].loc[2019,:]\n",
    "LM=regular_season_data_2019.groupby([\"Season\",\"LTeamID\"]).sum()[\"Margin\"].loc[2019,:]\n",
    "WBlk=regular_season_data_2019.groupby([\"Season\",\"WTeamID\"]).sum()[\"WBlk\"].loc[2019,:]\n",
    "LBlk=regular_season_data_2019.groupby([\"Season\",\"LTeamID\"]).sum()[\"LBlk\"].loc[2019,:]\n",
    "WAst=regular_season_data_2019.groupby([\"Season\",\"WTeamID\"]).sum()[\"WAst\"].loc[2019,:]\n",
    "LAst=regular_season_data_2019.groupby([\"Season\",\"LTeamID\"]).sum()[\"LAst\"].loc[2019,:]\n",
    "HW=regular_season_data_2019.groupby([\"Season\",\"WTeamID\",\"WLoc\"]).count()[\"DayNum\"].loc[2019,:,'H']\n",
    "HL=regular_season_data_2019.groupby([\"Season\",\"LTeamID\",\"WLoc\"]).count()[\"DayNum\"].loc[2019,:,'A']\n",
    "AW=regular_season_data_2019.groupby([\"Season\",\"WTeamID\",\"WLoc\"]).count()[\"DayNum\"].loc[2019,:,'A']\n",
    "AL=regular_season_data_2019.groupby([\"Season\",\"LTeamID\",\"WLoc\"]).count()[\"DayNum\"].loc[2019,:,'H']\n",
    "NW=regular_season_data_2019.groupby([\"Season\",\"WTeamID\",\"WLoc\"]).count()[\"DayNum\"].loc[2019,:,'N']\n",
    "NL=regular_season_data_2019.groupby([\"Season\",\"LTeamID\",\"WLoc\"]).count()[\"DayNum\"].loc[2019,:,'N']\n",
    "WTS=regular_season_data_2019.groupby([\"Season\",\"WTeamID\"]).sum()[\"WTS\"].loc[2019,:]\n",
    "LTS=regular_season_data_2019.groupby([\"Season\",\"LTeamID\"]).sum()[\"LTS\"].loc[2019,:]\n",
    "WTR=regular_season_data_2019.groupby([\"Season\",\"WTeamID\"]).sum()[\"WTR\"].loc[2019,:]\n",
    "LTR=regular_season_data_2019.groupby([\"Season\",\"LTeamID\"]).sum()[\"LTR\"].loc[2019,:]\n",
    "WOR=regular_season_data_2019.groupby([\"Season\",\"WTeamID\"]).sum()[\"WOR\"].loc[2019,:]\n",
    "LOR=regular_season_data_2019.groupby([\"Season\",\"LTeamID\"]).sum()[\"LOR\"].loc[2019,:]\n",
    "WStl=regular_season_data_2019.groupby([\"Season\",\"WTeamID\"]).sum()[\"WStl\"].loc[2019,:]\n",
    "LStl=regular_season_data_2019.groupby([\"Season\",\"LTeamID\"]).sum()[\"LStl\"].loc[2019,:]\n",
    "WTOVP=regular_season_data_2019.groupby([\"Season\",\"WTeamID\"]).sum()[\"WTOVP\"].loc[2019,:]\n",
    "LTOVP=regular_season_data_2019.groupby([\"Season\",\"LTeamID\"]).sum()[\"LTOVP\"].loc[2019,:]\n",
    "\n",
    "HW.index.names=['TeamID']\n",
    "HL.index.names=['TeamID']\n",
    "\n",
    "\n",
    "WM.index.names=['TeamID']\n",
    "LM.index.names=['TeamID']\n",
    "WM=pd.DataFrame(WM)\n",
    "LM=pd.DataFrame(LM)\n",
    "M_result=pd.merge(WM,LM,on=\"TeamID\",how='outer').fillna(0)\n",
    "M_result.columns=[\"WM\",\"LM\"]\n",
    "M_result[\"Total_Margin\"]=M_result[\"WM\"]-M_result[\"LM\"]\n",
    "M_result.drop('WM',axis=1,inplace=True)\n",
    "M_result.drop('LM',axis=1,inplace=True)\n",
    "\n",
    "#Total shooting percentage\n",
    "WTS.index.names=['TeamID']\n",
    "LTS.index.names=['TeamID']\n",
    "WTS=pd.DataFrame(WTS)\n",
    "LTS=pd.DataFrame(LTS)\n",
    "TS_result=pd.merge(WTS,LTS,on=\"TeamID\",how='outer').fillna(0)\n",
    "TS_result.columns=[\"WTS\",\"LTS\"]\n",
    "TS_result[\"Total_TS\"]=TS_result[\"WTS\"]+TS_result[\"LTS\"]\n",
    "TS_result.drop('WTS',axis=1,inplace=True)\n",
    "TS_result.drop('LTS',axis=1,inplace=True)\n",
    "\n",
    "#Total Rebound \n",
    "WTR.index.names=['TeamID']\n",
    "LTR.index.names=['TeamID']\n",
    "WTR=pd.DataFrame(WTR)\n",
    "LTR=pd.DataFrame(LTR)\n",
    "TR_result=pd.merge(WTR,LTR,on=\"TeamID\",how='outer').fillna(0)\n",
    "TR_result.columns=[\"WTR\",\"LTR\"]\n",
    "TR_result[\"Total_TR\"]=TR_result[\"WTR\"]+TR_result[\"LTR\"]\n",
    "TR_result.drop('WTR',axis=1,inplace=True)\n",
    "TR_result.drop('LTR',axis=1,inplace=True)\n",
    "\n",
    "#Offensive Rebound \n",
    "WOR.index.names=['TeamID']\n",
    "LOR.index.names=['TeamID']\n",
    "WOR=pd.DataFrame(WOR)\n",
    "LOR=pd.DataFrame(LOR)\n",
    "OR_result=pd.merge(WOR,LOR,on=\"TeamID\",how='outer').fillna(0)\n",
    "OR_result.columns=[\"WOR\",\"LOR\"]\n",
    "OR_result[\"Total_OR\"]=OR_result[\"WOR\"]+OR_result[\"LOR\"]\n",
    "OR_result.drop('WOR',axis=1,inplace=True)\n",
    "OR_result.drop('LOR',axis=1,inplace=True)\n",
    "\n",
    "#Steal \n",
    "WStl.index.names=['TeamID']\n",
    "LStl.index.names=['TeamID']\n",
    "WStl=pd.DataFrame(WStl)\n",
    "LStl=pd.DataFrame(LStl)\n",
    "Stl_result=pd.merge(WStl,LStl,on=\"TeamID\",how='outer').fillna(0)\n",
    "Stl_result.columns=[\"WStl\",\"LStl\"]\n",
    "Stl_result[\"Total_Stl\"]=Stl_result[\"WStl\"]+Stl_result[\"LStl\"]\n",
    "Stl_result.drop('WStl',axis=1,inplace=True)\n",
    "Stl_result.drop('LStl',axis=1,inplace=True)\n",
    "\n",
    "#Turn over percentage\n",
    "WTOVP.index.names=['TeamID']\n",
    "LTOVP.index.names=['TeamID']\n",
    "WTOVP=pd.DataFrame(WTOVP)\n",
    "LTOVP=pd.DataFrame(LTOVP)\n",
    "TOVP_result=pd.merge(WTOVP,LTOVP,on=\"TeamID\",how='outer').fillna(0)\n",
    "TOVP_result.columns=[\"WTOVP\",\"LTOVP\"]\n",
    "TOVP_result[\"Total_TOVP\"]=TOVP_result[\"WTOVP\"]+TOVP_result[\"LTOVP\"]\n",
    "TOVP_result.drop('WTOVP',axis=1,inplace=True)\n",
    "TOVP_result.drop('LTOVP',axis=1,inplace=True)\n",
    "\n",
    "WBlk.index.names=['TeamID']\n",
    "LBlk.index.names=['TeamID']\n",
    "WBlk=pd.DataFrame(WBlk)\n",
    "LBlk=pd.DataFrame(LBlk)\n",
    "Blk_result=pd.merge(WBlk,LBlk,on=\"TeamID\",how='outer').fillna(0)\n",
    "Blk_result.columns=[\"WBlk\",\"LBlk\"]\n",
    "Blk_result[\"Total_Blk\"]=Blk_result[\"WBlk\"]-Blk_result[\"LBlk\"]\n",
    "Blk_result.drop('WBlk',axis=1,inplace=True)\n",
    "Blk_result.drop('LBlk',axis=1,inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "WAst.index.names=['TeamID']\n",
    "LAst.index.names=['TeamID']\n",
    "WAst=pd.DataFrame(WAst)\n",
    "LAst=pd.DataFrame(LAst)\n",
    "Ast_result=pd.merge(WAst,LAst,on=\"TeamID\",how='outer').fillna(0)\n",
    "Ast_result.columns=[\"WAst\",\"LAst\"]\n",
    "Ast_result[\"Total_Ast\"]=Ast_result[\"WAst\"]+Ast_result[\"LAst\"]\n",
    "Ast_result.drop('WAst',axis=1,inplace=True)\n",
    "Ast_result.drop('LAst',axis=1,inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "HW=pd.DataFrame(HW)\n",
    "HL=pd.DataFrame(HL)\n",
    "H_result=pd.merge(HW,HL,on=\"TeamID\",how='outer').fillna(0)\n",
    "H_result.columns=[\"H_W\",\"H_L\"]\n",
    "H_result[\"H_Win_per\"]=(H_result[\"H_W\"])/(H_result[\"H_W\"]+H_result[\"H_L\"])\n",
    "AW.index.names=['TeamID']\n",
    "AL.index.names=['TeamID']\n",
    "AW=pd.DataFrame(AW)\n",
    "AL=pd.DataFrame(AL)\n",
    "A_result=pd.merge(AW,AL,on=\"TeamID\",how='outer').fillna(0)\n",
    "A_result.columns=[\"A_W\",\"A_L\"]\n",
    "A_result[\"A_Win_per\"]=(A_result[\"A_W\"])/(A_result[\"A_W\"]+A_result[\"A_L\"])\n",
    "NW.index.names=[ 'TeamID']\n",
    "NL.index.names=[ 'TeamID']\n",
    "NW=pd.DataFrame(NW)\n",
    "NL=pd.DataFrame(NL)\n",
    "N_result=pd.merge(NW,NL,on=\"TeamID\",how='outer').fillna(0)\n",
    "N_result.columns=[\"N_W\",\"N_L\"]\n",
    "N_result[\"N_Win_per\"]=(N_result[\"N_W\"])/(N_result[\"N_W\"]+N_result[\"N_L\"])\n",
    "Standings_2019=pd.concat([H_result,A_result,N_result,M_result,Ast_result,Blk_result,TS_result,TR_result,OR_result,Stl_result,TOVP_result], axis=1).fillna(0)\n",
    "Standings_2019[\"Season\"]=2019\n",
    "\n",
    "Standings_2019[\"T_Game\"]=Standings_2019[\"H_W\"]+Standings_2019[\"H_L\"]+Standings_2019[\"A_W\"]+Standings_2019[\"A_L\"]+Standings_2019[\"N_W\"]+Standings_2019[\"N_L\"]\n",
    "Standings_2019[\"T_Win\"]=Standings_2019[\"H_W\"]+Standings_2019[\"A_W\"]+Standings_2019[\"N_W\"]\n",
    "Standings_2019[\"T_Lose\"]=Standings_2019[\"H_L\"]+Standings_2019[\"A_L\"]+Standings_2019[\"N_L\"]\n",
    "Standings_2019[\"T_WIn_Per\"]=Standings_2019[\"T_Win\"]/Standings_2019[\"T_Game\"]\n",
    "Standings_2019[\"Margin_per_game\"]=Standings_2019[\"Total_Margin\"]/Standings_2019[\"T_Game\"]\n",
    "Standings_2019[\"Ast_per_game\"]=Standings_2019[\"Total_Ast\"]/Standings_2019[\"T_Game\"]\n",
    "Standings_2019[\"Blk_per_game\"]=Standings_2019[\"Total_Blk\"]/Standings_2019[\"T_Game\"]\n",
    "Standings_2019[\"TS_per_game\"]=Standings_2019[\"Total_TS\"]/Standings_2019[\"T_Game\"]\n",
    "Standings_2019[\"TR_per_game\"]=Standings_2019[\"Total_TR\"]/Standings_2019[\"T_Game\"]\n",
    "Standings_2019[\"OR_per_game\"]=Standings_2019[\"Total_OR\"]/Standings_2019[\"T_Game\"]\n",
    "Standings_2019[\"Stl_per_game\"]=Standings_2019[\"Total_Stl\"]/Standings_2019[\"T_Game\"]\n",
    "Standings_2019[\"TOVP_per_game\"]=Standings_2019[\"Total_TOVP\"]/Standings_2019[\"T_Game\"]\n",
    "\n",
    "Teams_2019=Teams_2019.sort_values(by=[\"TeamID\"])\n",
    "Teams_2019.index=Standings_2019.index\n",
    "Teams_2019=Teams_2019.drop(columns=[\"TeamID\"],axis=1)\n",
    "Final_Standings_2019=pd.concat([Teams_2019,Standings_2019],axis=1)\n",
    "Final_Standings_2019=Final_Standings_2019.sort_values(by=['Season',\"T_WIn_Per\",\"T_Win\"], ascending=False)\n",
    "Final_Standings_2019[\"TeamID\"]=Final_Standings_2019.index\n",
    "Final_Standings_2019.index=range(len(Final_Standings_2019))\n",
    "#전체승률 가지고 3개의 티어로 구분\n",
    "Final_Standings_2019[\"Tier\"]=pd.cut(Final_Standings_2019[\"T_WIn_Per\"],4,labels=[4,3,2,1])\n",
    "Standing_2019=Final_Standings_2019\n",
    "\n",
    "\n",
    "Standing_2019.drop('H_W',axis=1,inplace=True)\n",
    "Standing_2019.drop('H_L',axis=1,inplace=True)\n",
    "Standing_2019.drop('A_W',axis=1,inplace=True)\n",
    "Standing_2019.drop('A_L',axis=1,inplace=True)\n",
    "Standing_2019.drop('N_W',axis=1,inplace=True)\n",
    "Standing_2019.drop('N_L',axis=1,inplace=True)\n",
    "Standing_2019.drop('T_Game',axis=1,inplace=True)\n",
    "Standing_2019.drop('T_Win',axis=1,inplace=True)\n",
    "Standing_2019.drop('T_Lose',axis=1,inplace=True)\n",
    "\n",
    "Standing_2019.drop('N_Win_per',axis=1,inplace=True)\n",
    "Standing_2019.drop(\"Total_Margin\",axis=1,inplace=True)\n",
    "Standing_2019.drop(\"Total_Blk\",axis=1,inplace=True)\n",
    "Standing_2019.drop(\"Total_Ast\",axis=1,inplace=True)\n",
    "Standing_2019.drop(\"Total_TS\",axis=1,inplace=True)\n",
    "Standing_2019.drop(\"Total_TR\",axis=1,inplace=True)\n",
    "Standing_2019.drop(\"Total_OR\",axis=1,inplace=True)\n",
    "Standing_2019.drop(\"Total_Stl\",axis=1,inplace=True)\n",
    "Standing_2019.drop(\"Total_TOVP\",axis=1,inplace=True)\n",
    "\n",
    "\n",
    "a_2019=list(NCAASeed[NCAASeed[\"Season\"]==2019][\"TeamID\"])\n",
    "c_2019=pd.DataFrame(list(combinations(a_2019,2)))\n",
    "c_2019.columns=['TeamID1','TeamID2']\n",
    "t=c_2019.min(axis=1)\n",
    "h=c_2019.max(axis=1)\n",
    "c_2019[\"TeamID1\"]=t\n",
    "c_2019[\"TeamID2\"]=h\n",
    "c_2019=c_2019.sort_values(by=[\"TeamID1\",'TeamID2'])\n",
    "c_2019[\"Season\"]=np.zeros(len(c_2019[\"TeamID1\"]))\n",
    "c_2019=pd.DataFrame(c_2019)\n",
    "c_2019=c_2019.replace(0,2019)\n",
    "\n",
    "\n",
    "c_init.index=range(len(c_init[\"Season\"]))\n",
    "Standing_2019=Standing_2019.sort_values([\"Season\",\"TeamID\"])\n",
    "Standing_2019=pd.merge(Standing_2019,NCAASeed)\n",
    "\n",
    "Standing_2019.rename(columns={\"TeamID\":\"TeamID1\"}, inplace = True)\n",
    "Standing_1_2019=Standing_2019\n",
    "Final_2019=pd.merge(Standing_1_2019,c_2019)\n",
    "Final_2019.rename(columns={\n",
    "\"Stl_per_game\":\"T1_Stl_per_game\",\"TOVP_per_game\":\"T1_TOVP_per_game\",\"OR_per_game\":\"T1_OR_per_game\",\"TR_per_game\":\"T1_TR_per_game\",\"TS_per_game\":\"T1_TS_per_game\",\n",
    "\"Blk_per_game\":\"T1_Blk_per_game\",\"Ast_per_game\":\"T1_Ast_per_game\",\"Margin_per_game\":\"T1_Margin_per_game\",\"Seed\":\"T1_Seed\",\"H_Win_per\":\"T1_H_Win_per\",\"A_Win_per\":\"T1_A_Win_per\",\"T_WIn_Per\":\"T1_T_WIn_Per\",\"Tier\":\"T1_Tier\"}, inplace = True)\n",
    "Standing_1_2019.rename(columns={\"TeamID1\":\"TeamID2\"}, inplace = True)\n",
    "\n",
    "\n",
    "Final_2019=pd.merge(Final_2019,Standing_1_2019)\n",
    "Final_2019.rename(columns={\n",
    "\"Stl_per_game\":\"T2_Stl_per_game\",\"TOVP_per_game\":\"T2_TOVP_per_game\",\"OR_per_game\":\"T2_OR_per_game\",\"TR_per_game\":\"T2_TR_per_game\",\"TS_per_game\":\"T2_TS_per_game\",\n",
    "\"Blk_per_game\":\"T2_Blk_per_game\",\"Ast_per_game\":\"T2_Ast_per_game\",\"Margin_per_game\":\"T2_Margin_per_game\",\"Seed\":\"T2_Seed\",\"H_Win_per\":\"T2_H_Win_per\",\"A_Win_per\":\"T2_A_Win_per\",\"T_WIn_Per\":\"T2_T_WIn_Per\",\"Tier\":\"T2_Tier\"}, inplace = True)\n",
    "Final_2019=Final_2019.sort_values([\"Season\",\"TeamID1\",'TeamID2'])\n",
    "\n",
    "Final_2019.index=range(len(Final_2019[\"TeamID1\"]))\n",
    "Final_2019.drop('TeamID1',axis=1,inplace=True)\n",
    "Final_2019.drop('TeamID2',axis=1,inplace=True)\n",
    "Final_2019[\"T1_T_WIn_Per\"]=Final_2019[\"T1_T_WIn_Per\"].astype(np.float64)\n",
    "Final_2019[\"T2_T_WIn_Per\"]=Final_2019[\"T2_T_WIn_Per\"].astype(np.float64)\n",
    "Final_2019[\"T1_Seed\"]=Final_2019[\"T1_Seed\"].astype(np.float64)\n",
    "Final_2019[\"T2_Seed\"]=Final_2019[\"T2_Seed\"].astype(np.float64)\n",
    "Final_2019[\"T1_Tier\"]=Final_2019[\"T1_Tier\"].astype(np.float64)\n",
    "Final_2019[\"T2_Tier\"]=Final_2019[\"T2_Tier\"].astype(np.float64)\n",
    "Final_2019['T1_Margin_per_game']=Final_2019[\"T1_Margin_per_game\"].astype(np.float64)\n",
    "Final_2019['T2_Margin_per_game']=Final_2019[\"T2_Margin_per_game\"].astype(np.float64)\n",
    "Final_2019['T1_Ast_per_game']=Final_2019[\"T1_Ast_per_game\"].astype(np.float64)\n",
    "Final_2019['T2_Ast_per_game']=Final_2019[\"T2_Ast_per_game\"].astype(np.float64)\n",
    "Final_2019['T1_TS_per_game']=Final_2019[\"T1_TS_per_game\"].astype(np.float64)\n",
    "Final_2019['T2_TS_per_game']=Final_2019[\"T2_TS_per_game\"].astype(np.float64)\n",
    "Final_2019['T1_TR_per_game']=Final_2019[\"T1_TR_per_game\"].astype(np.float64)\n",
    "Final_2019['T2_TR_per_game']=Final_2019[\"T2_TR_per_game\"].astype(np.float64)\n",
    "Final_2019['T1_OR_per_game']=Final_2019[\"T1_OR_per_game\"].astype(np.float64)\n",
    "Final_2019['T2_OR_per_game']=Final_2019[\"T2_OR_per_game\"].astype(np.float64)\n",
    "Final_2019['T1_Stl_per_game']=Final_2019[\"T1_Stl_per_game\"].astype(np.float64)\n",
    "Final_2019['T2_Stl_per_game']=Final_2019[\"T2_Stl_per_game\"].astype(np.float64)\n",
    "Final_2019['T1_TOVP_per_game']=Final_2019[\"T1_TOVP_per_game\"].astype(np.float64)\n",
    "Final_2019['T2_TOVP_per_game']=Final_2019[\"T2_TOVP_per_game\"].astype(np.float64)\n",
    "Final_2019.drop(\"Season\",axis=1,inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=Final\n",
    "y=Output\n",
    "Train=pd.DataFrame(pd.concat([X,y],axis=1))\n",
    "Train[\"type\"]=np.zeros(len(Train[\"T1_Seed\"]))\n",
    "Train[Train[\"T1_Seed\"]==Train[\"T2_Seed\"]][\"type\"]='same'\n",
    "Train[(Train[\"T1_Seed\"]<Train[\"T2_Seed\"]) & (Train[\"Output\"]==0)][\"type\"]='upset'\n",
    "Train[(Train[\"T1_Seed\"]>Train[\"T2_Seed\"]) & (Train[\"Output\"]==1)][\"type\"]='upset'\n",
    "\n",
    "Train[(Train[\"T1_Seed\"]<Train[\"T2_Seed\"]) & (Train[\"Output\"]==1)][\"type\"]='not_upset'\n",
    "Train[(Train[\"T1_Seed\"]>Train[\"T2_Seed\"]) & (Train[\"Output\"]==0)][\"type\"]='not_upset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train.loc[(Train[\"T1_Seed\"]>Train[\"T2_Seed\"]) & (Train[\"Output\"]==0),[\"type\"]]='not_upset'\n",
    "Train.loc[(Train[\"T1_Seed\"]<Train[\"T2_Seed\"]) & (Train[\"Output\"]==1),[\"type\"]]='not_upset'\n",
    "Train.loc[(Train[\"T1_Seed\"]>Train[\"T2_Seed\"]) & (Train[\"Output\"]==1),[\"type\"]]='upset'\n",
    "Train.loc[(Train[\"T1_Seed\"]<Train[\"T2_Seed\"]) & (Train[\"Output\"]==0),[\"type\"]]='upset'\n",
    "Train.loc[Train[\"T1_Seed\"]==Train[\"T2_Seed\"],[\"type\"]]='same'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train.drop(\"Output\",axis=1,inplace=True)\n",
    "Train.to_csv(\"C:/Users/samsung/Desktop/Train.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>T1_H_Win_per</th>\n",
       "      <th>T1_A_Win_per</th>\n",
       "      <th>T1_T_WIn_Per</th>\n",
       "      <th>T1_Margin_per_game</th>\n",
       "      <th>T1_Ast_per_game</th>\n",
       "      <th>T1_Blk_per_game</th>\n",
       "      <th>T1_TS_per_game</th>\n",
       "      <th>T1_TR_per_game</th>\n",
       "      <th>T1_OR_per_game</th>\n",
       "      <th>T1_Stl_per_game</th>\n",
       "      <th>...</th>\n",
       "      <th>T2_Ast_per_game</th>\n",
       "      <th>T2_Blk_per_game</th>\n",
       "      <th>T2_TS_per_game</th>\n",
       "      <th>T2_TR_per_game</th>\n",
       "      <th>T2_OR_per_game</th>\n",
       "      <th>T2_Stl_per_game</th>\n",
       "      <th>T2_TOVP_per_game</th>\n",
       "      <th>T2_Tier</th>\n",
       "      <th>T2_Seed</th>\n",
       "      <th>Output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>567.000000</td>\n",
       "      <td>567.000000</td>\n",
       "      <td>567.000000</td>\n",
       "      <td>567.000000</td>\n",
       "      <td>567.000000</td>\n",
       "      <td>567.000000</td>\n",
       "      <td>567.000000</td>\n",
       "      <td>567.000000</td>\n",
       "      <td>567.000000</td>\n",
       "      <td>567.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>567.000000</td>\n",
       "      <td>567.000000</td>\n",
       "      <td>567.000000</td>\n",
       "      <td>567.000000</td>\n",
       "      <td>567.000000</td>\n",
       "      <td>567.000000</td>\n",
       "      <td>567.000000</td>\n",
       "      <td>567.000000</td>\n",
       "      <td>567.000000</td>\n",
       "      <td>567.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.863060</td>\n",
       "      <td>0.706954</td>\n",
       "      <td>0.797056</td>\n",
       "      <td>14.744860</td>\n",
       "      <td>15.928872</td>\n",
       "      <td>3.071691</td>\n",
       "      <td>0.531000</td>\n",
       "      <td>40.610811</td>\n",
       "      <td>13.818983</td>\n",
       "      <td>8.867454</td>\n",
       "      <td>...</td>\n",
       "      <td>14.970691</td>\n",
       "      <td>2.738696</td>\n",
       "      <td>0.522257</td>\n",
       "      <td>40.223833</td>\n",
       "      <td>13.762307</td>\n",
       "      <td>8.690698</td>\n",
       "      <td>17.900655</td>\n",
       "      <td>1.490300</td>\n",
       "      <td>6.213404</td>\n",
       "      <td>0.523810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.156226</td>\n",
       "      <td>0.185389</td>\n",
       "      <td>0.118012</td>\n",
       "      <td>9.009122</td>\n",
       "      <td>2.786306</td>\n",
       "      <td>1.697093</td>\n",
       "      <td>0.036971</td>\n",
       "      <td>3.162942</td>\n",
       "      <td>1.931212</td>\n",
       "      <td>1.953188</td>\n",
       "      <td>...</td>\n",
       "      <td>2.139042</td>\n",
       "      <td>1.309039</td>\n",
       "      <td>0.031019</td>\n",
       "      <td>2.915869</td>\n",
       "      <td>2.084913</td>\n",
       "      <td>1.939054</td>\n",
       "      <td>2.028694</td>\n",
       "      <td>0.514278</td>\n",
       "      <td>4.407506</td>\n",
       "      <td>0.499874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.468750</td>\n",
       "      <td>-5.593750</td>\n",
       "      <td>10.156250</td>\n",
       "      <td>-0.535714</td>\n",
       "      <td>0.439463</td>\n",
       "      <td>31.437500</td>\n",
       "      <td>8.593750</td>\n",
       "      <td>3.580645</td>\n",
       "      <td>...</td>\n",
       "      <td>9.862069</td>\n",
       "      <td>-0.125000</td>\n",
       "      <td>0.432047</td>\n",
       "      <td>30.733333</td>\n",
       "      <td>5.900000</td>\n",
       "      <td>4.066667</td>\n",
       "      <td>11.735376</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.812500</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.709677</td>\n",
       "      <td>7.948157</td>\n",
       "      <td>13.935484</td>\n",
       "      <td>1.812500</td>\n",
       "      <td>0.504542</td>\n",
       "      <td>38.343750</td>\n",
       "      <td>12.433333</td>\n",
       "      <td>7.433333</td>\n",
       "      <td>...</td>\n",
       "      <td>13.609375</td>\n",
       "      <td>1.781250</td>\n",
       "      <td>0.500415</td>\n",
       "      <td>38.257353</td>\n",
       "      <td>12.315887</td>\n",
       "      <td>7.381048</td>\n",
       "      <td>16.567477</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.794118</td>\n",
       "      <td>13.312500</td>\n",
       "      <td>15.375000</td>\n",
       "      <td>2.766667</td>\n",
       "      <td>0.529527</td>\n",
       "      <td>40.687500</td>\n",
       "      <td>13.870968</td>\n",
       "      <td>8.818182</td>\n",
       "      <td>...</td>\n",
       "      <td>14.933333</td>\n",
       "      <td>2.529412</td>\n",
       "      <td>0.522048</td>\n",
       "      <td>40.468750</td>\n",
       "      <td>13.870968</td>\n",
       "      <td>8.612903</td>\n",
       "      <td>17.715171</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.947222</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.878788</td>\n",
       "      <td>19.312500</td>\n",
       "      <td>17.303030</td>\n",
       "      <td>4.064516</td>\n",
       "      <td>0.555459</td>\n",
       "      <td>42.606061</td>\n",
       "      <td>15.015625</td>\n",
       "      <td>10.193548</td>\n",
       "      <td>...</td>\n",
       "      <td>16.193548</td>\n",
       "      <td>3.507576</td>\n",
       "      <td>0.540847</td>\n",
       "      <td>42.281250</td>\n",
       "      <td>15.187500</td>\n",
       "      <td>9.933333</td>\n",
       "      <td>19.138613</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>42.060606</td>\n",
       "      <td>23.343750</td>\n",
       "      <td>8.235294</td>\n",
       "      <td>0.627691</td>\n",
       "      <td>51.545455</td>\n",
       "      <td>18.468750</td>\n",
       "      <td>13.903226</td>\n",
       "      <td>...</td>\n",
       "      <td>23.343750</td>\n",
       "      <td>8.235294</td>\n",
       "      <td>0.619674</td>\n",
       "      <td>49.733333</td>\n",
       "      <td>19.733333</td>\n",
       "      <td>13.903226</td>\n",
       "      <td>24.731490</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       T1_H_Win_per  T1_A_Win_per  T1_T_WIn_Per  T1_Margin_per_game  \\\n",
       "count    567.000000    567.000000    567.000000          567.000000   \n",
       "mean       0.863060      0.706954      0.797056           14.744860   \n",
       "std        0.156226      0.185389      0.118012            9.009122   \n",
       "min        0.000000      0.200000      0.468750           -5.593750   \n",
       "25%        0.812500      0.571429      0.709677            7.948157   \n",
       "50%        0.882353      0.727273      0.794118           13.312500   \n",
       "75%        0.947222      0.846154      0.878788           19.312500   \n",
       "max        1.000000      1.000000      1.000000           42.060606   \n",
       "\n",
       "       T1_Ast_per_game  T1_Blk_per_game  T1_TS_per_game  T1_TR_per_game  \\\n",
       "count       567.000000       567.000000      567.000000      567.000000   \n",
       "mean         15.928872         3.071691        0.531000       40.610811   \n",
       "std           2.786306         1.697093        0.036971        3.162942   \n",
       "min          10.156250        -0.535714        0.439463       31.437500   \n",
       "25%          13.935484         1.812500        0.504542       38.343750   \n",
       "50%          15.375000         2.766667        0.529527       40.687500   \n",
       "75%          17.303030         4.064516        0.555459       42.606061   \n",
       "max          23.343750         8.235294        0.627691       51.545455   \n",
       "\n",
       "       T1_OR_per_game  T1_Stl_per_game     ...      T2_Ast_per_game  \\\n",
       "count      567.000000       567.000000     ...           567.000000   \n",
       "mean        13.818983         8.867454     ...            14.970691   \n",
       "std          1.931212         1.953188     ...             2.139042   \n",
       "min          8.593750         3.580645     ...             9.862069   \n",
       "25%         12.433333         7.433333     ...            13.609375   \n",
       "50%         13.870968         8.818182     ...            14.933333   \n",
       "75%         15.015625        10.193548     ...            16.193548   \n",
       "max         18.468750        13.903226     ...            23.343750   \n",
       "\n",
       "       T2_Blk_per_game  T2_TS_per_game  T2_TR_per_game  T2_OR_per_game  \\\n",
       "count       567.000000      567.000000      567.000000      567.000000   \n",
       "mean          2.738696        0.522257       40.223833       13.762307   \n",
       "std           1.309039        0.031019        2.915869        2.084913   \n",
       "min          -0.125000        0.432047       30.733333        5.900000   \n",
       "25%           1.781250        0.500415       38.257353       12.315887   \n",
       "50%           2.529412        0.522048       40.468750       13.870968   \n",
       "75%           3.507576        0.540847       42.281250       15.187500   \n",
       "max           8.235294        0.619674       49.733333       19.733333   \n",
       "\n",
       "       T2_Stl_per_game  T2_TOVP_per_game     T2_Tier     T2_Seed      Output  \n",
       "count       567.000000        567.000000  567.000000  567.000000  567.000000  \n",
       "mean          8.690698         17.900655    1.490300    6.213404    0.523810  \n",
       "std           1.939054          2.028694    0.514278    4.407506    0.499874  \n",
       "min           4.066667         11.735376    1.000000    1.000000    0.000000  \n",
       "25%           7.381048         16.567477    1.000000    3.000000    0.000000  \n",
       "50%           8.612903         17.715171    1.000000    5.000000    1.000000  \n",
       "75%           9.933333         19.138613    2.000000    9.000000    1.000000  \n",
       "max          13.903226         24.731490    3.000000   16.000000    1.000000  \n",
       "\n",
       "[8 rows x 27 columns]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X_train, y_train, X_val, y_val, X_test 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.2 ,random_state = 42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler  \n",
    "scaler = StandardScaler()  \n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train_scale = scaler.transform(X_train)  \n",
    "X_val_scale = scaler.transform(X_val) \n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "\n",
    "error_minkowski = []\n",
    "error_manhattan = []\n",
    "error_euclidean = []\n",
    "error_mahalanobis = []\n",
    "\n",
    "# Calculating error for K values between 1 and 20\n",
    "for i in range(1, 21):  \n",
    "    knn = KNeighborsClassifier(n_neighbors=i)\n",
    "    knn.fit(X_train_scale, y_train)\n",
    "    pred_i = knn.predict_proba(X_val_scale)\n",
    "    error_minkowski.append(log_loss(y_val, pred_i))\n",
    "\n",
    "for i in range(1, 21):  \n",
    "    knn = KNeighborsClassifier(n_neighbors=i,metric='manhattan')\n",
    "    knn.fit(X_train_scale, y_train)\n",
    "    pred_i = knn.predict_proba(X_val_scale)\n",
    "    error_manhattan.append(log_loss(y_val, pred_i))\n",
    "\n",
    "for i in range(1, 21):  \n",
    "    knn = KNeighborsClassifier(n_neighbors=i,metric='euclidean')\n",
    "    knn.fit(X_train_scale, y_train)\n",
    "    pred_i = knn.predict_proba(X_val_scale)\n",
    "    error_euclidean.append(log_loss(y_val, pred_i))\n",
    "\n",
    "for i in range(1, 21):  \n",
    "    cov = np.cov(X_train_scale, rowvar=False)\n",
    "    knn = KNeighborsClassifier(n_neighbors=i,metric='mahalanobis', metric_params=dict(V=cov))\n",
    "    knn.fit(X_train_scale, y_train)\n",
    "    pred_i = knn.predict_proba(X_val_scale)\n",
    "    error_mahalanobis.append(log_loss(y_val, pred_i))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "    \n",
    "plt.figure(figsize=(12, 6))  \n",
    "plt.plot(range(1, 21), error_minkowski, color='red', linestyle='dashed', marker='o',  \n",
    "         markerfacecolor='blue', markersize=10,label=\"a\")\n",
    "\n",
    "plt.plot(range(1, 21), error_manhattan, color='yellow', linestyle='dashed', marker='o',  \n",
    "         markerfacecolor='blue', markersize=10,label=\"a\")\n",
    "\n",
    "plt.plot(range(1, 21), error_euclidean, color='green', linestyle='dashed', marker='o',  \n",
    "         markerfacecolor='blue', markersize=10,label=\"a\")\n",
    "\n",
    "plt.plot(range(1, 21), error_mahalanobis, color='black', linestyle='dashed', marker='o',  \n",
    "         markerfacecolor='blue', markersize=10,label=\"a\")\n",
    "plt.legend(['minkowski', 'manhattan', 'euclidean','mahalanobis'], loc='upper left')\n",
    "plt.title('log_loss K Value')  \n",
    "plt.xlabel('K Value')  \n",
    "plt.ylabel('log_loss')  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier log_loss is 0.581\n"
     ]
    }
   ],
   "source": [
    "# 최적의 k와 distance measure\n",
    "\n",
    "knn_final = KNeighborsClassifier(n_neighbors=6,metric='manhattan')\n",
    "knn.fit(X_train_scale, y_train)\n",
    "pred_knn = knn.predict_proba(X_val_scale)\n",
    "print(knn.__class__.__name__+\" log_loss is %2.3f\" % log_loss(y_val, pred_knn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "나이브 베이즈 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GaussianNB log_loss is 0.946\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "NB = GaussianNB()\n",
    "NB.fit(X_train_scale, y_train)\n",
    "pred_NB = NB.predict_proba(X_val_scale)\n",
    "print(NB.__class__.__name__+\" log_loss is %2.3f\" % log_loss(y_val, pred_NB))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "로지스틱 리그레션"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression log_loss is 0.440\n"
     ]
    }
   ],
   "source": [
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred_proba = logreg.predict_proba(X_val)\n",
    "print(logreg.__class__.__name__+\" log_loss is %2.3f\" % log_loss(y_val, y_pred_proba))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "로지스틱 리그레션 parameter tuning 시도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV log_loss is 0.444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed:    2.7s finished\n"
     ]
    }
   ],
   "source": [
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "pipe = Pipeline([('classifier' , RandomForestClassifier())])\n",
    "param_grid = [\n",
    "    {'classifier' : [LogisticRegression()],\n",
    "     'classifier__penalty' : ['l1', 'l2' ],\n",
    "    'classifier__C' : np.logspace(-4, 4, 20),\n",
    "    'classifier__solver' : ['liblinear']}\n",
    "]\n",
    "\n",
    "clf = GridSearchCV(pipe, param_grid = param_grid, cv = 5, verbose=True, n_jobs=-1)\n",
    "# Fit on data\n",
    "logreg = clf.fit(X_train, y_train)\n",
    "y_pred_proba_best = logreg.predict_proba(X_val)\n",
    "print(logreg.__class__.__name__+\" log_loss is %2.3f\" % log_loss(y_val, y_pred_proba_best))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'classifier': [LogisticRegression(C=0.012742749857031334, class_weight=None, dual=False,\n",
       "             fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "             multi_class='warn', n_jobs=None, penalty='l1', random_state=None,\n",
       "             solver='liblinear', tol=0.0001, verbose=0, warm_start=False)],\n",
       "  'classifier__penalty': ['l1', 'l2'],\n",
       "  'classifier__C': array([1.00000000e-04, 2.63665090e-04, 6.95192796e-04, 1.83298071e-03,\n",
       "         4.83293024e-03, 1.27427499e-02, 3.35981829e-02, 8.85866790e-02,\n",
       "         2.33572147e-01, 6.15848211e-01, 1.62377674e+00, 4.28133240e+00,\n",
       "         1.12883789e+01, 2.97635144e+01, 7.84759970e+01, 2.06913808e+02,\n",
       "         5.45559478e+02, 1.43844989e+03, 3.79269019e+03, 1.00000000e+04]),\n",
       "  'classifier__solver': ['liblinear']}]"
      ]
     },
     "execution_count": 379,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support vector classify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC log_loss is 0.562\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "svm=svm.SVC(probability=True)\n",
    "svc=svm.fit(X_train, y_train)\n",
    "y_pred_proba_best = svc.predict_proba(X_val)\n",
    "print(svc.__class__.__name__+\" log_loss is %2.3f\" % log_loss(y_val, y_pred_proba_best))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 225 candidates, totalling 1125 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done 232 tasks      | elapsed:    4.9s\n",
      "[Parallel(n_jobs=-1)]: Done 588 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done 849 tasks      | elapsed:  4.3min\n",
      "[Parallel(n_jobs=-1)]: Done 1125 out of 1125 | elapsed:  8.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV log_loss is 0.465\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "pipe = Pipeline([('classifier' , RandomForestClassifier())])\n",
    "param_grid = [\n",
    "    {'classifier' : [svm.SVC(probability=True)],\n",
    "     'classifier__kernel' : ['linear', 'rbf', 'poly'],\n",
    "    'classifier__C' : [0.001, 0.01, 0.1, 1, 10],\n",
    "    'classifier__gamma' : [0.001, 0.01, 0.1, 1,10],\n",
    "    'classifier__degree' : [ 1, 2, 3]\n",
    "    }\n",
    "]\n",
    "\n",
    "clf = GridSearchCV(pipe, param_grid = param_grid, cv = 5, verbose=True, n_jobs=-1)\n",
    "svm = clf.fit(X_train, y_train)\n",
    "y_pred_proba_best = svm.predict_proba(X_val)\n",
    "print(svm.__class__.__name__+\" log_loss is %2.3f\" % log_loss(y_val, y_pred_proba_best))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'classifier': [SVC(C=0.001, cache_size=200, class_weight=None, coef0=0.0,\n",
       "     decision_function_shape='ovr', degree=1, gamma=0.001, kernel='linear',\n",
       "     max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
       "     tol=0.001, verbose=False)],\n",
       "  'classifier__kernel': ['linear', 'rbf', 'poly'],\n",
       "  'classifier__C': [0.001, 0.01, 0.1, 1, 10],\n",
       "  'classifier__gamma': [0.001, 0.01, 0.1, 1, 10],\n",
       "  'classifier__degree': [1, 2, 3]}]"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Desicion Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier log_loss is 11.210\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "DT = DecisionTreeClassifier()\n",
    "DT.fit(X_train, y_train)\n",
    "y_pred_proba = DT.predict_proba(X_val)\n",
    "print(DT.__class__.__name__+\" log_loss is %2.3f\" % log_loss(y_val, y_pred_proba))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 240 candidates, totalling 1200 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done 336 tasks      | elapsed:    0.6s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV log_loss is 0.565\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 1200 out of 1200 | elapsed:    1.4s finished\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline([('classifier' , RandomForestClassifier())])\n",
    "param_grid = [\n",
    "    {'classifier' : [DecisionTreeClassifier()],\n",
    "     'classifier__criterion' : ['gini', 'entropy'],\n",
    "    'classifier__max_depth' : [5,10,20,30,50,100],\n",
    "    'classifier__min_samples_leaf' : [3,4,5,6,7],\n",
    "    'classifier__min_samples_split' : [6, 8, 12, 15]\n",
    "    }\n",
    "]\n",
    "\n",
    "clf = GridSearchCV(pipe, param_grid = param_grid, cv = 5, verbose=True, n_jobs=-1)\n",
    "DT = clf.fit(X_train, y_train)\n",
    "y_pred_proba_best = DT.predict_proba(X_val)\n",
    "print(DT.__class__.__name__+\" log_loss is %2.3f\" % log_loss(y_val, y_pred_proba_best))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'classifier': [DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=5,\n",
       "               max_features=None, max_leaf_nodes=None,\n",
       "               min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "               min_samples_leaf=7, min_samples_split=12,\n",
       "               min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "               splitter='best')],\n",
       "  'classifier__criterion': ['gini', 'entropy'],\n",
       "  'classifier__max_depth': [5, 10, 20, 30, 50, 100],\n",
       "  'classifier__min_samples_leaf': [3, 4, 5, 6, 7],\n",
       "  'classifier__min_samples_split': [6, 8, 12, 15]}]"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import  export_graphviz\n",
    "import pydot\n",
    "DT = DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=5,\n",
    "               max_features=None, max_leaf_nodes=None,\n",
    "               min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "               min_samples_leaf=7, min_samples_split=12,\n",
    "               min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
    "               splitter='best')\n",
    "DT.fit(X_train, y_train)\n",
    "export_graphviz(DT, out_file=\"dicisionTree1.dot\", class_names=[\"A\",\"B\",\"C\"],\n",
    "                feature_names=X_train.columns, impurity=False, filled=True)\n",
    "\n",
    "(graph,) = pydot.graph_from_dot_file('dicisionTree1.dot', encoding='utf8')\n",
    "\n",
    "#Dot 파일을 Png 이미지로 저장\n",
    "graph.write_png('dicisionTree1.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AdaBoostClassfier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoostClassifier log_loss is 0.605\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "ada = AdaBoostClassifier()\n",
    "ada.fit(X_train, y_train)\n",
    "y_pred_proba_ada = ada.predict_proba(X_val).clip(0,1)\n",
    "print(ada.__class__.__name__+\" log_loss is %2.3f\" % log_loss(y_val, y_pred_proba_ada))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AdaBoostClassfier parameter tuning 시도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.5, 'n_estimators': 100}"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crossvalidation=KFold(n_splits=10,shuffle=True,random_state=1)\n",
    "search_grid={'n_estimators':[5,10,20,50,100],'learning_rate':[0.007,0.01,0.05,0.1,0.2,0.5]}\n",
    "search=GridSearchCV(estimator=ada,param_grid=search_grid,scoring='neg_log_loss',n_jobs=1,cv=crossvalidation)\n",
    "search.fit(X_train,y_train)\n",
    "search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoostClassifier log_loss is 0.547\n"
     ]
    }
   ],
   "source": [
    "ada_tuned = AdaBoostClassifier(learning_rate= 0.5, n_estimators= 10)\n",
    "ada_tuned.fit(X_train, y_train)\n",
    "y_pred_proba_ada_tuned = ada_tuned.predict_proba(X_val).clip(0,1)\n",
    "print(ada_tuned.__class__.__name__+\" log_loss is %2.3f\" % log_loss(y_val, y_pred_proba_ada_tuned))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientBoostingClassifier log_loss is 0.478\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier \n",
    "GB = GradientBoostingClassifier()\n",
    "GB.fit(X_train, y_train)\n",
    "y_pred_proba_GB = GB.predict_proba(X_val)\n",
    "print(GB.__class__.__name__+\" log_loss is %2.3f\" % log_loss(y_val, y_pred_proba_GB))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GradientBoostingClassifier parameter tuning 시도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.01, 'n_estimators': 300}"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crossvalidation=KFold(n_splits=10,shuffle=True,random_state=1)\n",
    "search_grid={'n_estimators':[10,50,100,150,300],'learning_rate':[0.007,0.01,0.05,0.1,0.2,0.5]}\n",
    "search=GridSearchCV(estimator=GB,param_grid=search_grid,scoring='neg_log_loss',n_jobs=1,cv=crossvalidation)\n",
    "search.fit(X_train,y_train)\n",
    "search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientBoostingClassifier log_loss is 0.458\n"
     ]
    }
   ],
   "source": [
    "GB_tuned = GradientBoostingClassifier(learning_rate= 0.01, n_estimators= 300)\n",
    "GB_tuned.fit(X_train, y_train)\n",
    "y_pred_proba_GB_tuned = GB_tuned.predict_proba(X_val)\n",
    "print(GB_tuned.__class__.__name__+\" log_loss is %2.3f\" % log_loss(y_val, y_pred_proba_GB_tuned))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LinearDiscriminantAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearDiscriminantAnalysis log_loss is 0.457\n"
     ]
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "LDA= LinearDiscriminantAnalysis()\n",
    "LDA.fit(X_train, y_train)\n",
    "y_pred_proba_LDA = LDA.predict_proba(X_val)\n",
    "print(LDA.__class__.__name__+\" log_loss is %2.3f\" % log_loss(y_val, y_pred_proba_LDA))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier log_loss is 0.573\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "RF = RandomForestClassifier()\n",
    "RF.fit(X_train, y_train)\n",
    "y_pred_proba = RF.predict_proba(X_val)\n",
    "print(RF.__class__.__name__+\" log_loss is %2.3f\" % log_loss(y_val, y_pred_proba))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 384 candidates, totalling 1920 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    5.4s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   22.2s\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:   54.1s\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 1234 tasks      | elapsed:  2.7min\n",
      "[Parallel(n_jobs=-1)]: Done 1784 tasks      | elapsed:  4.1min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV log_loss is 0.454\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 1920 out of 1920 | elapsed:  4.4min finished\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline([('classifier' , RandomForestClassifier())])\n",
    "param_grid = [\n",
    "    {'classifier' : [RandomForestClassifier()],\n",
    "\n",
    "     'classifier__n_estimators' : [100,200,300,1000],\n",
    "     'classifier__criterion' : ['gini', 'entropy'],\n",
    "    'classifier__max_depth' : [5,10,20,30],\n",
    "    'classifier__min_samples_leaf' : [3,4,5,6],\n",
    "    'classifier__min_samples_split' : [8, 12, 15]\n",
    "    }\n",
    "]\n",
    "\n",
    "clf = GridSearchCV(pipe, param_grid = param_grid, cv = 5, verbose=True, n_jobs=-1)\n",
    "RF = clf.fit(X_train, y_train)\n",
    "y_pred_proba_best = RF.predict_proba(X_val)\n",
    "print(RF.__class__.__name__+\" log_loss is %2.3f\" % log_loss(y_val, y_pred_proba_best))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'classifier': [RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "               max_depth=5, max_features='auto', max_leaf_nodes=None,\n",
       "               min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "               min_samples_leaf=6, min_samples_split=12,\n",
       "               min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None,\n",
       "               oob_score=False, random_state=None, verbose=0,\n",
       "               warm_start=False)],\n",
       "  'classifier__n_estimators': [100, 200, 300, 1000],\n",
       "  'classifier__criterion': ['gini', 'entropy'],\n",
       "  'classifier__max_depth': [5, 10, 20, 30],\n",
       "  'classifier__min_samples_leaf': [3, 4, 5, 6],\n",
       "  'classifier__min_samples_split': [8, 12, 15]}]"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앙상블기법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR: (-0.443) +/- (0.066)\n",
      "SVM: (-0.442) +/- (0.069)\n",
      "GB: (-0.502) +/- (0.082)\n",
      "LDA: (-0.456) +/- (0.088)\n",
      "VotingClassifier log_loss is 0.433\n"
     ]
    }
   ],
   "source": [
    "seed = 7 \n",
    "processors=1 \n",
    "num_folds=10\n",
    "num_instances=len(X) \n",
    "score='neg_log_loss'\n",
    "kfold =KFold(n_splits=num_folds, random_state=seed)\n",
    "models = [] \n",
    "models.append(('LR',  LogisticRegression(C=0.004832930238571752, class_weight=None, dual=False,\n",
    "             fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
    "             multi_class='warn', n_jobs=None, penalty='l2', random_state=None,\n",
    "             solver='liblinear', tol=0.0001, verbose=0, warm_start=False))) \n",
    "models.append(('SVM', SVC(C=0.001, cache_size=200, class_weight=None, coef0=0.0,\n",
    "     decision_function_shape='ovr', degree=1, gamma=0.001, kernel='linear',\n",
    "     max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
    "     tol=0.001, verbose=False)))\n",
    "models.append(('GB',  GradientBoostingClassifier(learning_rate= 0.01, n_estimators= 300)))  \n",
    "models.append(('LDA',LinearDiscriminantAnalysis())) \n",
    "results = [] \n",
    "names = [] \n",
    "for name, model in models:     \n",
    "    cv_results = cross_val_score(model, X_train, y_train, cv=kfold, scoring=score, n_jobs=processors)     \n",
    "    results.append(cv_results)     \n",
    "    names.append(name)     \n",
    "    print(\"{0}: ({1:.3f}) +/- ({2:.3f})\".format(name, cv_results.mean(), cv_results.std()))\n",
    "    \n",
    "    \n",
    "estimators = [] \n",
    "estimators.append(('LR', LogisticRegression(C=0.004832930238571752, class_weight=None, dual=False,\n",
    "             fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
    "             multi_class='warn', n_jobs=None, penalty='l2', random_state=None,\n",
    "             solver='liblinear', tol=0.0001, verbose=0, warm_start=False))) \n",
    "estimators.append(('SVM', SVC(C=0.001, cache_size=200, class_weight=None, coef0=0.0,\n",
    "     decision_function_shape='ovr', degree=1, gamma=0.001, kernel='linear',\n",
    "     max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
    "     tol=0.001, verbose=False))) \n",
    "estimators.append(('GB', GradientBoostingClassifier(learning_rate= 0.01, n_estimators= 300)))\n",
    "estimators.append(('LDA', LinearDiscriminantAnalysis())) \n",
    "# create the ensemble model \n",
    "ensemble = VotingClassifier(estimators, voting='soft', weights=[1,1,1,1])\n",
    "ensemble.fit(X_train, y_train) \n",
    "y_pred_proba_essemble = ensemble.predict_proba(X_val)\n",
    "print(ensemble.__class__.__name__+\" log_loss is %2.3f\" % log_loss(y_val, y_pred_proba_essemble))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for loop를 사용해 최적 weight 찾기\n",
    "a=[1,3,9,27,81]\n",
    "index_df=pd.DataFrame(columns=[\"i\",\"j\",\"k\",\"m\"])\n",
    "log_loss_=[]\n",
    "for i in range(5):\n",
    "    for j in range(5):\n",
    "        for k in range(5):\n",
    "            for m in range(5):\n",
    "                ensemble = VotingClassifier(estimators, voting='soft', weights=[a[i],a[j],a[k],a[m]])\n",
    "                ensemble.fit(X_train, y_train) \n",
    "                y_pred_proba_essemble = ensemble.predict_proba(X_val)\n",
    "                log_loss_.append(log_loss(y_val, y_pred_proba_essemble))\n",
    "                index_df_add = pd.DataFrame(np.transpose(pd.DataFrame([i,j,k,m])),columns=[\"i\",\"j\",\"k\",\"m\"])\n",
    "                index_df=pd.concat([index_df,index_df_add])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "124\n",
      "0.43080690940903493\n",
      "VotingClassifier log_loss is 0.431\n",
      "VotingClassifier log_loss is 0.496\n"
     ]
    }
   ],
   "source": [
    "print(log_loss_.index(min(log_loss_)))   #최적의 weight --> 1,81,81,81\n",
    "print(min(log_loss_))\n",
    "\n",
    "ensemble = VotingClassifier(estimators, voting='soft', weights=[1,81,81,81])\n",
    "ensemble.fit(X_train, y_train) \n",
    "y_pred_proba_essemble = ensemble.predict_proba(X_val)\n",
    "print(ensemble.__class__.__name__+\" log_loss is %2.3f\" % log_loss(y_val, y_pred_proba_essemble))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-logloss:0.688782\tvalid-logloss:0.689536\n",
      "Multiple eval metrics have been passed: 'valid-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-logloss hasn't improved in 100 rounds.\n",
      "[100]\ttrain-logloss:0.448893\tvalid-logloss:0.515236\n",
      "[200]\ttrain-logloss:0.356076\tvalid-logloss:0.472452\n",
      "[300]\ttrain-logloss:0.300976\tvalid-logloss:0.458731\n",
      "[400]\ttrain-logloss:0.262266\tvalid-logloss:0.450408\n",
      "[500]\ttrain-logloss:0.229527\tvalid-logloss:0.446123\n",
      "[600]\ttrain-logloss:0.202644\tvalid-logloss:0.443833\n",
      "[700]\ttrain-logloss:0.18104\tvalid-logloss:0.44565\n",
      "Stopping. Best iteration:\n",
      "[638]\ttrain-logloss:0.194365\tvalid-logloss:0.441697\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "def run_xgb(train_X, train_y, val_X, val_y, test_X):\n",
    "    params = {'objective': 'binary:logistic', \n",
    "          'eval_metric': 'logloss',\n",
    "          'eta': 0.01,\n",
    "              'n_estimators': 1000,\n",
    "          'max_depth': 3, \n",
    "          'subsample': 0.8, \n",
    "          'colsample_bytree': 1,\n",
    "          'alpha':0.01,\n",
    "        'gamma':1,\n",
    "          'random_state': 42, \n",
    "          'silent': True}\n",
    "    \n",
    "    tr_data = xgb.DMatrix(train_X, train_y)\n",
    "    va_data = xgb.DMatrix(val_X, val_y)\n",
    "    \n",
    "    watchlist = [(tr_data, 'train'), (va_data, 'valid')]\n",
    "    \n",
    "    model_xgb = xgb.train(params, tr_data, 2000, watchlist, maximize=False, early_stopping_rounds = 100, verbose_eval=100)\n",
    "    \n",
    "    dtest = xgb.DMatrix(test_X)\n",
    "    xgb_pred_y = model_xgb.predict(dtest, ntree_limit=model_xgb.best_ntree_limit)\n",
    "    \n",
    "    return xgb_pred_y, model_xgb\n",
    "\n",
    "\n",
    "pred_test_xgb, model_xgb = run_xgb(X_train, y_train, X_val, y_val, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Booster log_loss is 0.540\n"
     ]
    }
   ],
   "source": [
    "print(model_xgb.__class__.__name__+\" log_loss is %2.3f\" % log_loss(y_test, pred_test_xgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " model_xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "\n",
    "params = {'objective' : \"binary\", \n",
    "               'boost':\"gbdt\",\n",
    "               'metric':\"binary_logloss\",\n",
    "               'boost_from_average':\"false\",\n",
    "               'num_threads':8,\n",
    "               'learning_rate' : 0.01,\n",
    "               'num_leaves' : 8,\n",
    "               'max_depth':3,\n",
    "               'tree_learner' : \"serial\",\n",
    "               'feature_fraction' : 0.05,\n",
    "               'bagging_freq' : 5,\n",
    "               'bagging_fraction' : 0.4,\n",
    "               'min_data_in_leaf' : 80,\n",
    "               'min_sum_hessian_in_leaf' : 10.0,\n",
    "               'verbosity' : 1}\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 started at Sun Jun  2 22:47:22 2019\n",
      "Training until validation scores don't improve for 3500 rounds.\n",
      "[5000]\ttraining's binary_logloss: 0.433338\tvalid_1's binary_logloss: 0.357951\n",
      "[10000]\ttraining's binary_logloss: 0.402283\tvalid_1's binary_logloss: 0.335418\n",
      "[15000]\ttraining's binary_logloss: 0.383191\tvalid_1's binary_logloss: 0.325125\n",
      "[20000]\ttraining's binary_logloss: 0.368002\tvalid_1's binary_logloss: 0.321397\n",
      "Early stopping, best iteration is:\n",
      "[16513]\ttraining's binary_logloss: 0.378014\tvalid_1's binary_logloss: 0.313983\n",
      "Fold 1 started at Sun Jun  2 22:47:40 2019\n",
      "Training until validation scores don't improve for 3500 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[184]\ttraining's binary_logloss: 0.604488\tvalid_1's binary_logloss: 0.678962\n",
      "Fold 2 started at Sun Jun  2 22:47:43 2019\n",
      "Training until validation scores don't improve for 3500 rounds.\n",
      "[5000]\ttraining's binary_logloss: 0.422262\tvalid_1's binary_logloss: 0.534624\n",
      "[10000]\ttraining's binary_logloss: 0.389784\tvalid_1's binary_logloss: 0.532531\n",
      "Early stopping, best iteration is:\n",
      "[7241]\ttraining's binary_logloss: 0.405313\tvalid_1's binary_logloss: 0.515664\n",
      "Fold 3 started at Sun Jun  2 22:47:52 2019\n",
      "Training until validation scores don't improve for 3500 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[263]\ttraining's binary_logloss: 0.588166\tvalid_1's binary_logloss: 0.671496\n",
      "Fold 4 started at Sun Jun  2 22:47:57 2019\n",
      "Training until validation scores don't improve for 3500 rounds.\n",
      "[5000]\ttraining's binary_logloss: 0.426972\tvalid_1's binary_logloss: 0.500829\n",
      "[10000]\ttraining's binary_logloss: 0.395478\tvalid_1's binary_logloss: 0.511576\n",
      "Early stopping, best iteration is:\n",
      "[6938]\ttraining's binary_logloss: 0.411955\tvalid_1's binary_logloss: 0.488442\n",
      "Fold 5 started at Sun Jun  2 22:48:07 2019\n",
      "Training until validation scores don't improve for 3500 rounds.\n",
      "[5000]\ttraining's binary_logloss: 0.423674\tvalid_1's binary_logloss: 0.491969\n",
      "Early stopping, best iteration is:\n",
      "[6012]\ttraining's binary_logloss: 0.415574\tvalid_1's binary_logloss: 0.483993\n",
      "Fold 6 started at Sun Jun  2 22:48:15 2019\n",
      "Training until validation scores don't improve for 3500 rounds.\n",
      "[5000]\ttraining's binary_logloss: 0.42428\tvalid_1's binary_logloss: 0.478004\n",
      "Early stopping, best iteration is:\n",
      "[3792]\ttraining's binary_logloss: 0.438201\tvalid_1's binary_logloss: 0.473409\n",
      "Fold 7 started at Sun Jun  2 22:48:21 2019\n",
      "Training until validation scores don't improve for 3500 rounds.\n",
      "[5000]\ttraining's binary_logloss: 0.426597\tvalid_1's binary_logloss: 0.509518\n",
      "Early stopping, best iteration is:\n",
      "[1695]\ttraining's binary_logloss: 0.482376\tvalid_1's binary_logloss: 0.494447\n",
      "Fold 8 started at Sun Jun  2 22:48:25 2019\n",
      "Training until validation scores don't improve for 3500 rounds.\n",
      "[5000]\ttraining's binary_logloss: 0.42584\tvalid_1's binary_logloss: 0.457469\n",
      "[10000]\ttraining's binary_logloss: 0.394061\tvalid_1's binary_logloss: 0.448785\n",
      "Early stopping, best iteration is:\n",
      "[8771]\ttraining's binary_logloss: 0.400187\tvalid_1's binary_logloss: 0.442208\n",
      "Fold 9 started at Sun Jun  2 22:48:31 2019\n",
      "Training until validation scores don't improve for 3500 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1364]\ttraining's binary_logloss: 0.502562\tvalid_1's binary_logloss: 0.406947\n",
      "Fold 10 started at Sun Jun  2 22:48:35 2019\n",
      "Training until validation scores don't improve for 3500 rounds.\n",
      "[5000]\ttraining's binary_logloss: 0.427981\tvalid_1's binary_logloss: 0.423289\n",
      "Early stopping, best iteration is:\n",
      "[4465]\ttraining's binary_logloss: 0.43402\tvalid_1's binary_logloss: 0.415315\n",
      "Fold 11 started at Sun Jun  2 22:48:42 2019\n",
      "Training until validation scores don't improve for 3500 rounds.\n",
      "[5000]\ttraining's binary_logloss: 0.426579\tvalid_1's binary_logloss: 0.418993\n",
      "Early stopping, best iteration is:\n",
      "[5702]\ttraining's binary_logloss: 0.420957\tvalid_1's binary_logloss: 0.413842\n",
      "Fold 12 started at Sun Jun  2 22:48:50 2019\n",
      "Training until validation scores don't improve for 3500 rounds.\n",
      "[5000]\ttraining's binary_logloss: 0.4245\tvalid_1's binary_logloss: 0.495013\n",
      "Early stopping, best iteration is:\n",
      "[4857]\ttraining's binary_logloss: 0.42601\tvalid_1's binary_logloss: 0.492817\n",
      "Fold 13 started at Sun Jun  2 22:48:56 2019\n",
      "Training until validation scores don't improve for 3500 rounds.\n",
      "[5000]\ttraining's binary_logloss: 0.431805\tvalid_1's binary_logloss: 0.387941\n",
      "[10000]\ttraining's binary_logloss: 0.401355\tvalid_1's binary_logloss: 0.361198\n",
      "[15000]\ttraining's binary_logloss: 0.381091\tvalid_1's binary_logloss: 0.35845\n",
      "Early stopping, best iteration is:\n",
      "[12815]\ttraining's binary_logloss: 0.389272\tvalid_1's binary_logloss: 0.352613\n",
      "Fold 14 started at Sun Jun  2 22:49:05 2019\n",
      "Training until validation scores don't improve for 3500 rounds.\n",
      "[5000]\ttraining's binary_logloss: 0.432189\tvalid_1's binary_logloss: 0.404548\n",
      "[10000]\ttraining's binary_logloss: 0.402414\tvalid_1's binary_logloss: 0.384117\n",
      "[15000]\ttraining's binary_logloss: 0.382248\tvalid_1's binary_logloss: 0.375946\n",
      "Early stopping, best iteration is:\n",
      "[14403]\ttraining's binary_logloss: 0.384447\tvalid_1's binary_logloss: 0.371597\n",
      "Fold 15 started at Sun Jun  2 22:49:15 2019\n",
      "Training until validation scores don't improve for 3500 rounds.\n",
      "[5000]\ttraining's binary_logloss: 0.42101\tvalid_1's binary_logloss: 0.581179\n",
      "[10000]\ttraining's binary_logloss: 0.390741\tvalid_1's binary_logloss: 0.600627\n",
      "Early stopping, best iteration is:\n",
      "[7427]\ttraining's binary_logloss: 0.403878\tvalid_1's binary_logloss: 0.574848\n",
      "Fold 16 started at Sun Jun  2 22:49:24 2019\n",
      "Training until validation scores don't improve for 3500 rounds.\n",
      "[5000]\ttraining's binary_logloss: 0.42944\tvalid_1's binary_logloss: 0.433565\n",
      "[10000]\ttraining's binary_logloss: 0.398474\tvalid_1's binary_logloss: 0.429632\n",
      "Early stopping, best iteration is:\n",
      "[7919]\ttraining's binary_logloss: 0.408463\tvalid_1's binary_logloss: 0.425353\n",
      "Fold 17 started at Sun Jun  2 22:49:34 2019\n",
      "Training until validation scores don't improve for 3500 rounds.\n",
      "[5000]\ttraining's binary_logloss: 0.419116\tvalid_1's binary_logloss: 0.596427\n",
      "Early stopping, best iteration is:\n",
      "[2289]\ttraining's binary_logloss: 0.460109\tvalid_1's binary_logloss: 0.567911\n",
      "Fold 18 started at Sun Jun  2 22:49:39 2019\n",
      "Training until validation scores don't improve for 3500 rounds.\n",
      "[5000]\ttraining's binary_logloss: 0.428467\tvalid_1's binary_logloss: 0.429624\n",
      "[10000]\ttraining's binary_logloss: 0.396137\tvalid_1's binary_logloss: 0.413257\n",
      "Early stopping, best iteration is:\n",
      "[10947]\ttraining's binary_logloss: 0.391496\tvalid_1's binary_logloss: 0.409992\n",
      "Fold 19 started at Sun Jun  2 22:49:52 2019\n",
      "Training until validation scores don't improve for 3500 rounds.\n",
      "[5000]\ttraining's binary_logloss: 0.43253\tvalid_1's binary_logloss: 0.350048\n",
      "[10000]\ttraining's binary_logloss: 0.400652\tvalid_1's binary_logloss: 0.324841\n",
      "[15000]\ttraining's binary_logloss: 0.380154\tvalid_1's binary_logloss: 0.315816\n",
      "Early stopping, best iteration is:\n",
      "[13099]\ttraining's binary_logloss: 0.387469\tvalid_1's binary_logloss: 0.312547\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "fold_n=20\n",
    "folds = StratifiedKFold(n_splits=fold_n, shuffle=True, random_state=10)\n",
    "\n",
    "\n",
    "y_pred_lgb = np.zeros(len(Final_2019))\n",
    "num_round = 10000000\n",
    "for fold_n, (train_index, valid_index) in enumerate(folds.split(X,y)):\n",
    "    print('Fold', fold_n, 'started at', time.ctime())\n",
    "    X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]\n",
    "    y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n",
    "    \n",
    "    train_data = lgb.Dataset(X_train, label=y_train)\n",
    "    valid_data = lgb.Dataset(X_valid, label=y_valid)\n",
    "        \n",
    "    lgb_model = lgb.train(params,train_data,num_round,#change 20 to 2000\n",
    "                    valid_sets = [train_data, valid_data],verbose_eval=5000,early_stopping_rounds = 3500)##change 10 to 200\n",
    "            \n",
    "    y_pred_lgb += lgb_model.predict(Final_2019, num_iteration=lgb_model.best_iteration)/20\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RandomForest 중요변수 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>T2_Seed</th>\n",
       "      <th>T1_Margin_per_game</th>\n",
       "      <th>T2_Margin_per_game</th>\n",
       "      <th>T1_A_Win_per</th>\n",
       "      <th>T1_Blk_per_game</th>\n",
       "      <th>T1_Seed</th>\n",
       "      <th>T2_TOVP_per_game</th>\n",
       "      <th>T2_TR_per_game</th>\n",
       "      <th>T2_T_WIn_Per</th>\n",
       "      <th>T1_TR_per_game</th>\n",
       "      <th>T1_Stl_per_game</th>\n",
       "      <th>T1_T_WIn_Per</th>\n",
       "      <th>T1_H_Win_per</th>\n",
       "      <th>T1_Ast_per_game</th>\n",
       "      <th>T2_TS_per_game</th>\n",
       "      <th>T1_OR_per_game</th>\n",
       "      <th>T1_TS_per_game</th>\n",
       "      <th>T1_TOVP_per_game</th>\n",
       "      <th>T2_Blk_per_game</th>\n",
       "      <th>T2_Stl_per_game</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>importance</th>\n",
       "      <td>0.156127</td>\n",
       "      <td>0.127244</td>\n",
       "      <td>0.073596</td>\n",
       "      <td>0.048201</td>\n",
       "      <td>0.043674</td>\n",
       "      <td>0.040776</td>\n",
       "      <td>0.040581</td>\n",
       "      <td>0.038868</td>\n",
       "      <td>0.036056</td>\n",
       "      <td>0.035496</td>\n",
       "      <td>0.033287</td>\n",
       "      <td>0.03238</td>\n",
       "      <td>0.032182</td>\n",
       "      <td>0.031705</td>\n",
       "      <td>0.029116</td>\n",
       "      <td>0.028542</td>\n",
       "      <td>0.027583</td>\n",
       "      <td>0.025902</td>\n",
       "      <td>0.025408</td>\n",
       "      <td>0.022029</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             T2_Seed  T1_Margin_per_game  T2_Margin_per_game  T1_A_Win_per  \\\n",
       "importance  0.156127            0.127244            0.073596      0.048201   \n",
       "\n",
       "            T1_Blk_per_game   T1_Seed  T2_TOVP_per_game  T2_TR_per_game  \\\n",
       "importance         0.043674  0.040776          0.040581        0.038868   \n",
       "\n",
       "            T2_T_WIn_Per  T1_TR_per_game  T1_Stl_per_game  T1_T_WIn_Per  \\\n",
       "importance      0.036056        0.035496         0.033287       0.03238   \n",
       "\n",
       "            T1_H_Win_per  T1_Ast_per_game  T2_TS_per_game  T1_OR_per_game  \\\n",
       "importance      0.032182         0.031705        0.029116        0.028542   \n",
       "\n",
       "            T1_TS_per_game  T1_TOVP_per_game  T2_Blk_per_game  T2_Stl_per_game  \n",
       "importance        0.027583          0.025902         0.025408         0.022029  "
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc_model = RandomForestClassifier() \n",
    "rfc_model.fit(X_train, y_train) \n",
    "feature_imp = pd.DataFrame(rfc_model.feature_importances_, index=X.columns, columns=[\"importance\"]) \n",
    "feat_imp_20 =pd.DataFrame(np.transpose(feature_imp.sort_values(\"importance\", ascending=False).head(20)))\n",
    "feat_imp_20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "feat_imp_20.plot(kind='bar',figsize=(20,10))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Permutation Importanace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "    table.eli5-weights tr:hover {\n",
       "        filter: brightness(85%);\n",
       "    }\n",
       "</style>\n",
       "\n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "        <table class=\"eli5-weights eli5-feature-importances\" style=\"border-collapse: collapse; border: none; margin-top: 0em; table-layout: auto;\">\n",
       "    <thead>\n",
       "    <tr style=\"border: none;\">\n",
       "        <th style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">Weight</th>\n",
       "        <th style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">Feature</th>\n",
       "    </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 80.00%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0825\n",
       "                \n",
       "                    &plusmn; 0.0361\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                T1_Seed\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 84.72%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0561\n",
       "                \n",
       "                    &plusmn; 0.0263\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                T2_Seed\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 87.51%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0421\n",
       "                \n",
       "                    &plusmn; 0.0406\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                T1_Margin_per_game\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 88.62%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0368\n",
       "                \n",
       "                    &plusmn; 0.0131\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                T1_Blk_per_game\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 89.39%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0333\n",
       "                \n",
       "                    &plusmn; 0.0233\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                T2_A_Win_per\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 90.59%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0281\n",
       "                \n",
       "                    &plusmn; 0.0258\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                T2_Stl_per_game\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 91.87%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0228\n",
       "                \n",
       "                    &plusmn; 0.0306\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                T2_Blk_per_game\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 92.76%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0193\n",
       "                \n",
       "                    &plusmn; 0.0131\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                T1_H_Win_per\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 92.76%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0193\n",
       "                \n",
       "                    &plusmn; 0.0131\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                T2_T_WIn_Per\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 93.23%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0175\n",
       "                \n",
       "                    &plusmn; 0.0111\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                T1_Stl_per_game\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 93.23%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0175\n",
       "                \n",
       "                    &plusmn; 0.0192\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                T1_Ast_per_game\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 93.71%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0158\n",
       "                \n",
       "                    &plusmn; 0.0070\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                T1_T_WIn_Per\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 94.21%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0140\n",
       "                \n",
       "                    &plusmn; 0.0179\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                T1_A_Win_per\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 94.21%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0140\n",
       "                \n",
       "                    &plusmn; 0.0086\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                T1_TS_per_game\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 94.73%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0123\n",
       "                \n",
       "                    &plusmn; 0.0238\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                T2_Margin_per_game\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 94.73%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0123\n",
       "                \n",
       "                    &plusmn; 0.0086\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                T1_TR_per_game\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 95.27%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0105\n",
       "                \n",
       "                    &plusmn; 0.0070\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                T2_TOVP_per_game\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 96.44%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0070\n",
       "                \n",
       "                    &plusmn; 0.0070\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                T2_H_Win_per\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 96.44%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0070\n",
       "                \n",
       "                    &plusmn; 0.0131\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                T2_Ast_per_game\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 97.81%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0035\n",
       "                \n",
       "                    &plusmn; 0.0086\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                T2_TS_per_game\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 98.65%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0018\n",
       "                \n",
       "                    &plusmn; 0.0070\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                T2_OR_per_game\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(0, 100.00%, 100.00%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0\n",
       "                \n",
       "                    &plusmn; 0.0000\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                T1_TOVP_per_game\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(0, 100.00%, 100.00%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0\n",
       "                \n",
       "                    &plusmn; 0.0000\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                T2_Tier\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(0, 100.00%, 100.00%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0\n",
       "                \n",
       "                    &plusmn; 0.0000\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                T2_TR_per_game\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(0, 100.00%, 100.00%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0\n",
       "                \n",
       "                    &plusmn; 0.0000\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                T1_Tier\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(0, 100.00%, 98.65%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                -0.0018\n",
       "                \n",
       "                    &plusmn; 0.0131\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                T1_OR_per_game\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "    \n",
       "    </tbody>\n",
       "</table>\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import eli5\n",
    "\n",
    "from eli5.sklearn import PermutationImportance\n",
    "perm = PermutationImportance(rfc_model, random_state=1).fit(X_val, y_val)\n",
    "eli5.show_weights(perm, feature_names = X_val.columns.tolist(), top=26)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Randomforest 기법을 통해 변수별 중요도를 파악하고 feature selection을 시도 했으나 validation set에대한 지표가 모든 변수를 사용하는것이 좋게 나와서 모든변수를 사용해서 예측파일을 만듭니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#앙상블기법 예측파일 만들기\n",
    "preds_e = ensemble.predict_proba(Final_2019) \n",
    "Submission=pd.read_csv(\"C:/Users/samsung/Desktop/WNCAA/WSampleSubmissionStage2.csv\")\n",
    "Submission[\"Pred\"]=preds_e\n",
    "Submission.index=Submission[\"ID\"]\n",
    "Submission.drop(\"ID\",axis=1,inplace=True)\n",
    "Submission.to_csv(\"C:/Users/samsung/Desktop/submission_Ensemble.csv\")\n",
    "\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
